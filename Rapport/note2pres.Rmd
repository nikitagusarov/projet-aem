---
title: "Etude des effets des pésticides dans la production des vins de table"
shorttitle : "Les effets des pésticides"
subtitle: "Analyse empirique des marchés"
author: Arnaud Blanc, Nikita Gusarov, Sasha Picon
shortauthor : A.Blanc, N.Gusarov, S.Picon
institute: Université Grenoble Alpes
shortinstitute: UGA
date: 25/12/2019  
# header-includes:
#     - \usepackage{array}
#     - \usepackage{multicol}
#     - \usepackage{graphicx}
#     - \usepackage{placeins}
#     - \usepackage{xcolor}
output: 
    pdf_document:
        # template: Template.tex
        # slide_level: 2
        # fonttheme: "structurebold"
        toc: false
        # toc_depth: 1
        df_print: "kable"
        fig_width: 6
        fig_height: 3
        fig_caption: yes
        number_sections: FALSE
        includes:
            in_header: packages.sty
            before_body: toc.sty
fontsize: 11pt
bibliography: biblio.bib
# geometry: margin = 0.5in
---

```{r include = FALSE}
###################
# Setting r options
###################
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(size = "tiny")
knitr::opts_chunk$set(dpi = 600)
knitr::opts_chunk$set(fig.align = "center") 
knitr::opts_chunk$set(fig.pos = "!htbp")
```

```{r include = FALSE, eval = FALSE}
#########################
# Extract R code from Rmd
#########################
# require(knitr)
# purl("./Presentation/Presentation.Rmd", 
#     output = "./Presentation/CodePresentation.R", 
#     documentation = 0)
```

```{r include = FALSE}
##########
# Packages
##########
# Results printing
require(stargazer)
require(texreg)
require(xtable)
# Statistics
require(systemfit) # System equations
require(olsrr) # Stat tests
require(plm) # Panel data
require(Formula)
# Plots 
require(gridExtra)
# Maps
require(raster)
# General packages
require(tidyverse)
require(rlang)
require(dummies)
require(dplyr)
```

```{r include = FALSE}
##############
# Loading data
##############
# Data
data = read.csv("../Donnees/Base-de-donnees-indice-prix.csv")
# names(data)
# Arrange
dn = data %>%
    filter(s_vin_simple != 0 & 
        (q_rouge + q_blanc) != 0 &
        (qk_prod + ql_prod) != 0 &
        IP != 0) %>%
    na.omit() %>%
    group_by(ndep) %>%
    count() %>% 
    filter(n == 5) # %>%
    # dplyr::select(ndep)
datax = data %>% 
    filter(ndep %in% dn$ndep) 
datay = datax %>% 
    filter(annee == 2012) %>%
    mutate(refqki = qk_prod + ql_prod) %>% 
    dplyr::select(ndep, refqki) 
datax = left_join(datax, datay)
datax = datax %>%
    mutate(IQK = (qk_prod + ql_prod)/refqki)
```

```{r include = FALSE}
##################
# Transformed data
##################
datai = datax %>%
    arrange(ndep) %>%
    mutate(si = log(s_vin_simple + 0.001), 
        qi = log(q_blanc + q_rouge + 0.001), 
        ipi = IP,
        ri = log(revenu.déflaté),
        iki = IQK,
        t = as.integer(as.factor(annee)),
        year = annee) %>%
    dplyr::select(year, ndep, qi, ipi, si, ri, iki, t)
```

```{r include = FALSE}
############
# Panel data
############
datap = pdata.frame(datai, index = c("ndep", "year"),
    drop.index = T)
```

```{r include = FALSE}
###################
# Support functions
###################
# STATA version
#######
# xtsum (overall, within and between variance) for panel data
#######
xtsum = function(data, varname, unit) {
    # the variable to xtsum over
    varname = enquo(varname)
    # the identifier dimention
    loc.unit = enquo(unit)
    # overall
    ores = data %>% 
        summarise(ovr.mean = mean(!! varname, na.rm = TRUE), 
        ovr.sd = sd(!! varname, na.rm = TRUE), 
        ovr.min = min(!! varname, na.rm = TRUE), 
        ovr.max = max(!! varname, na.rm = TRUE), 
        ovr.N = sum(as.numeric((!is.na(!! varname)))))
    # between
    bmeans = data %>% 
        group_by(!! loc.unit) %>% 
        summarise(meanx = mean(!! varname, na.rm = TRUE), 
        t.count = sum(as.numeric(!is.na(!! varname))))
    bres = bmeans %>% 
        ungroup() %>% 
        summarise(between.sd = sd(meanx, na.rm = TRUE), 
        between.min = min(meanx, na.rm = TRUE), 
        between.max = max(meanx, na.rm = TRUE), 
        units = sum(as.numeric(!is.na(t.count))), 
        t.bar = mean(t.count, na.rm = TRUE))
    # within
    wdat = data %>% 
        group_by(!! loc.unit) %>% 
        mutate(W.x = scale(!! varname, scale = FALSE))
    wres = wdat %>% 
        ungroup() %>%  
        summarise(within.sd = sd(W.x, na.rm = TRUE), 
        within.min = min(W.x, na.rm = TRUE), 
        within.max = max(W.x, na.rm = TRUE))
    # results
    return(list(var = varname, ores = ores, bres = bres, wres = wres))
}
####################################
# Print results for a list of xtsums
####################################
print.xtsum = function(xtsums.list) {
    # takes multiple xtsums as list
    df = data.frame(Variable = NA, Mean = NA,
        Overall = NA, Between = NA, Within = NA)
    # Filling loop
    for (i in 1:length(xtsums.list)) {
        df[i,1] = as_name(xtsums.list[[i]]$var)
        df[i,2] = xtsums.list[[i]]$ores$ovr.mean 
        df[i,3] = xtsums.list[[i]]$ores$ovr.sd
        df[i,4] = xtsums.list[[i]]$bres$between.sd
        df[i,5] = xtsums.list[[i]]$wres$within.sd
    }
    # Rownames
    rownames(df) = df[,1]
    # Results
    return(df = df[,-1])
}
#################
# Effects testing 
#################
Effect.testing = function(Formulas, data) {
    Dtest = data.frame(var = 0,
        Random = 0, Fixed = 0, 
        Individual = 0, Time = 0, Twoways = 0)
        for (i in 1:length(Formulas)) {
            Dtest[i,1] = names(Formulas)[i]
            ## Chow test
            # Random coefs for random effects          
            Dtest[i,2] = pooltest(Formulas[[i]],
                data = data,
                model = "random")$p.val 
            # Different coefs for fixed effects
            Dtest[i,3] = pooltest(Formulas[[i]],
                data = data,
                model = "within")$p.val
            ## Lagrange multiplier tests
            # Individual effects
            Dtest[i,4] = plmtest(Formulas[[i]],
                data = data,
                effect = "individual",
                type = "bp")$p.val
            # Time effects
            Dtest[i,5] = plmtest(Formulas[[i]],
                data = data,
                effect = "time",
                type = "bp")$p.val
            # Two-ways effects (individual and time)
            Dtest[i,6] = plmtest(Formulas[[i]],
                data = data,
                effect = "twoways",
                type = "ghm")$p.val   
        }
    rownames(Dtest) = Dtest[,1]
    return(Dtest = Dtest[,-1])
}
```

```{r include = FALSE}
##################
# Set ggplot style
##################
pres_theme = theme(text = element_text(size = rel(3)),
    legend.position = "none")
```

# Introduction

```{r eval = FALSE, include = FALSE}
####################################################
################### Introduction ###################
####################################################
```

Aujourd’hui, l’utilisation des pesticides est un problème majeur de l’agriculture.  
Celle-ci utilise la plus grande partie des pesticides en France. 
Il s’agit d’un enjeu à la base du développement durable car ils ont un impact important sur les risques environnementaux et sanitaires. 

Les pesticides sont utilisés dans l’agriculture pour protéger la production. 
Ils sont supposés protéger les rendements. 
En effet, les aléas climatiques influencent le développement de champignons ou de maladies. 
Ainsi, les pesticides permettent de protéger les cultures contre les aléas climatiques et de ne pas perdre de production. 

Dans ce travail nous cherchons à comprendre et à estimer les effets des pesticides sur le marché des vins simples.
De cette façon nous chercherons à étudier l'équilibre sur le marché des vins simples ce qui est sensé nous donner des résultats plus précis et fiables.

# 1. Les pesticides

```{r eval = FALSE, include = FALSE}
####################################################
###################  Pesticides  ###################
####################################################
```

\noindent\rule[0.5ex]{\linewidth}{1pt} 

\textcolor{red}{Mettre des sources partout !}

\noindent\rule[0.5ex]{\linewidth}{1pt}

Pour lutter contre l’utilisation des pesticides l’Etat Français et l’union européenne ont mis en place des mesures. 
Ainsi, l’Etat Français lors du grenelle de l’environnement de 2006 a fixé ses objectifs. 
Ainsi, le plan ECOPHYTO 2018 visait à réduire de 50% l’utilisation des pesticides de synthèse. 
Le deuxième objectif est le passage en agriculture biologique à 6% de la surface agricole utilisée en 2010 et vise 20% en 2020.[@Butault2014.bib]

En 2008, les 30 produits les plus toxiques les plus toxiques sont interdits. 
Une taxe sur les phytosanitaires a aussi été mise en place. 
Cette taxe est croissante avec le niveau de toxicité de ces produits. 
Cette taxe devait augmenter au fil des années. De plus, l’octroi de crédits d’impôt en faveur de l’agriculture biologique devait aussi permettre de réduire l'utilisation des pesticides.[@Butault2014.bib]

Malgré tous ces efforts, l’utilisation des pesticides perdurent.  
En 2008, le nombre de doses unités a été créé pour enregistrer l’évolution de la demande de pesticide.[@Butault2014.bib]
On remarque que les doses utilisées augmentent de 12% en 2014-2016 par rapport à 2009-2011.  

## Etat actuel

Contrairement aux attentes des autorités, on ne remarque aucune baisse de l’utilisation de pesticides. 
Le Nodu a connu une hausse de 23% entre 2008 et 2017. 
Certaines critiques ont été faites sur l’utilisation du Nodu. 
Il est possible d’utiliser le nombre de substances actives utilisées. 
Mais, cet indicateur connaît lui aussi une hausse de 15% entre 2011 et 2017. 

Néanmoins, les politiques ont quand même eu quelques effets positifs, puisque l’achat des produits les plus dangereux baisse de 6% en 2017. [@Moghaddam2019.bib] 
Les grandes cultures sont les premières utilisatrices de pesticides. 
Elles représentent 67,4% de l’utilisation de pesticides. 
La deuxième culture est celle de la vigne ce qui représente 14,4%  des pesticides utilisés.[@Butault2014.bib]

## Comment baisser l'utilisation de pesticides

Afin de baisser l’utilisation des pesticides, des méthodes de cultures ont été développées pour baisser l’utilisation des pesticides. 
Il est possible d’utiliser différents mode de culture. 
On peut en retenir trois principaux. 

Le premier est l’agriculture intensive. 
Elle ne limite pas le recours aux pesticides. 

Le deuxième est l’agriculture raisonnée. 
Elle limite le recours aux pesticides en fonction de seuils.
 
Le troisième niveau est l’agriculture biologique. 
Elle supprime les traitements avec des produits phytosanitaires de synthèse. 

Les professionnels proposent de commencer par utiliser l’agriculture raisonnée qui permet de réduire les doses de pesticides légales. 
Ensuite l’agriculture doit se déplacer vers l’agriculture biologique qui n’utilise aucun produit phytosanitaire de synthèse. 

# 2. Le marché du vin français

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Viticulture  ###################
#####################################################
```

La France est l’un des principaux producteurs de vins. En effet, la France représente 10% de la surface des vignes mondiales. La production de vins représentait 4.6 milliards  de litres. La France représentait 17% de la production totale de vins. 3% de la surface agricole française est consacrée à la production agricole. Néanmoins, le vin représente 15% de la production agricole en valeur. [@CNIV, 2018]
La France est aussi l’un des principaux consommateurs de vins. En effet, en France, il s’agit de la boisson alcoolisée la plus consommée. 88% des ventes de vins en France sont effectuées en grande surface. Néanmoins, la consommation française de vin baisse depuis une trentaine d’années. [@CNIV2018.bib] 

## Utilisation des pesticides dans la viticulture

La viticulture est le deuxième secteur agricole en termes d’utilisation des pesticides. En effet, elle représente plus de 14.4% des dépenses de produits phytosanitaires,  en France. Néanmoins, ces pesticides ne sont pas utilisés dans la même proportion dans toutes les régions de France. [@Butault2011.bib] 
Les bassins viticoles Français utilisent en majorité des fongicides et des bactéricides. En effet, la vigne fait face à des aléas climatiques qui permettent le développement de champignons comme le Mildiou. [@Pujol2017.bib] 
Pour lutter contre le développement de ces champignons, les viticulteurs ne peuvent utiliser que des fongicides. En effet, ils ne peuvent pas utiliser la rotation des cultures qui pourraient réduire ou empêcher le développement de ces champignons puisque la vigne est une culture pérenne. Les pieds de vigne ne sont pas replantés chaque année. Il est donc nécessaire d’utiliser les pesticides dans la vigne pour protéger la production et éviter les pertes. En effet, les champignons s’attaquent aux feuilles de la vigne et aux fruits. Donc la pulvérisation de pesticides est un des seuls moyens pour protéger les rendements des cultures viticoles.  Néanmoins, l’utilisation des pesticides a aussi un impact du côté de la demande de vin. Cet impact est plus ambigu, à cause d’un manque de transparence d’information sur les bouteilles de vin. [@Prudent2018.bib]
Un sondage de l’Ifop sur les habitudes et perceptions de consommation des Français a montré que 93% des Français considèrent que la présence de pesticides dans les aliments a un impact sur la santé. 89% des Français souhaiteraient être informés de la présence ou non de pesticides dans les produits alimentaires, à travers un étiquetage. [@Ifop2017.bib]

## Le problème d'heterogénéité

Le secteur du vin est constitué de produits qui sont fortement hétérogènes.
En effet, il existe une forte hétérogénéité entre les différents labels (AOP, IGP, sans IG) mais aussi au sein de ces labels. 

Dans le commerce du vin, il est courant de diviser les vins en deux grandes classes en fonction de leurs prix [@cembalo2014] : 

- les vins de qualité inférieure, les moins chers avec les caractéristiques de qualité de base ;
- les vins de qualité supérieure plus chers, dotés de caractéristiques qualitatives complexes et d'une image de grande valeur.

De plus, pour les vins français, selon @steiner2004, le système européen de classification des "*vins de qualité produits dans certaines régions*" (VQPRD) contient à la fois des vins AOC et des "*vins de haute qualité provenant d'un vignoble régional agréé*" (VDQS). 
Les vins de cépage appartiennent à la catégorie des vins autres que VQPRD, qui comprend les \textbf{vins de table} et les \textbf{vins de pays}.

En tenant compte des spécificités du marhcé du vin français, nous utilisons la méthodologie du ministère d'agriculture et divisons le marché en deux parties :

- La gamme haute (les vins IGP et AOP, vendus dans des magasins spécifiques) ;
- La gamme basse (les vins sans IG, vendus en grands surfaces).

La première partie est soumise à des règlements spécifiques : limitations des quantités produites, origine contrôlé, un caractère de la demande spécifique. 
La deuxième, c'est-à-dire le marché des vins moins chers, est aussi complexe. Néanmoins, elle demeure moins hétérogène @cembalo2014. En effet, les vins qui se situent dans une fourchette de prix étroite sont quasiment homogènes. Ainsi, les vins sans indication géographique ont des attributs intrinsèques simples, une complexité de qualité faible. Il s’agit donc de vins peu différenciés. Nous avons, donc, choisit de nous concentrer sur ces vins sans indication géographique à cause de leur degré d’homogénéité qui est plus fort que pour les autres labels.

Cela nous permet d'analyser le marché par département est non par des marques/produits.
## Les vins de table

Le marché des vins sans indication géographiques connaît de forte variation. Nous allons donc revenir sur la période qui précède notre étude. Ainsi, en 2011, les transactions de vente de vins rouges ont augmenté de 29%. Les transactions de vins rosés ont également augmenté de 13%. Pour finir, les transactions de vins blancs augmentaient de 76%. Les prix de ces vins bien que faible connaissent aussi des variations importantes. Ainsi, en 2011, les trois couleurs de vins ont connus des hausses de prix. Les vins rouges ont vu leurs prix moyens augmenté de 12%. Le prix moyens des vins rosés ont aussi crus de 3 %. Pour finir, les prix moyens des vins blancs ont cru de 13%. Les vins de France sans indications géographiques  ont connu une baisse en volume des ventes de 14.6% par rapport à la moyenne des ventes sur la période 2006 à 2010. [@FranceAgriMer2011.bib].

 


\noindent\rule[0.5ex]{\linewidth}{1pt}

\textcolor{red}{Ajouter des articles proches par la méthodologie à notre }

- \textcolor{red}{le cas du modèle simple,  }
- \textcolor{red}{le cas du marchéliée, } 
- \textcolor{red}{le cas des clusters.}

\noindent\rule[0.5ex]{\linewidth}{1pt}


# 3. Le cadre théorique

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Théorie mod  ###################
#####################################################
```

## Les hypothèses théoriques

\noindent\rule[0.5ex]{\linewidth}{1pt}

\textcolor{red}{Ajouter les réferences ...}

\noindent\rule[0.5ex]{\linewidth}{1pt}

Comme proposé dans la littérature, notre étude sur les vins non coûteux (non IGP) est effectué au niveau du pays @cembalo2014 pour deux raisons. 
D'abord, les prix de vente moyens des marchés sont diffèrents en raison des droits de douane à l'importation et des taxes à la consommation différentes [@anderson2011global].
De plus, la perception des produits de consommation varie d'un pays à l'autre [@makela2006].

Rachat du vin par les enseignes (grand surfaces) ... @kremer2004

La plupart des bouteilles achetées sont achetées dans la grande distribution. 
Néanmoins, dans un souci de simplicité nous estimerons que les consommateurs achètent leurs bouteilles directement auprès du viticulteur. 
Donc nous supprimerons tous les intermédiaires entre le producteur et le marché final.

Quand aux exportations et aux importations, n'ayant pas la possibilité contrôler le montant des vins non IGP exportés/importés, nous laissons ces effets au terme d'erreur. 
Nous ignorons complétement les interactions internationales. 

```{r include = FALSE}
# Il existe indéniablement, au sein de la grande distribution, une offre différenciée par format de vente : les hypermarchés se sont plutôt spécialisés dans le haut et le milieu de gamme avec une sur-représentation des VQPRD, les supermarchés ont privilégié les AOC à bas prix et les vins de table et de pays tandis que le hard discount se spécialisait dans le bas de gamme
```

Facteurs de production ... @laporte1996 

```{r include = FALSE} 
# Les variables explicatives retenues ont été regroupées en cinq catégories :
# - un indicateur du portefeuille d'appellations : son contenu a été présenté et largem ent commenté aux § 22 et 31 ;
# - un indicateur de stratégie technique : il s'agit soit du rendement, soit de la politique de rendem ent ;
# - un indicateur de stratégie commerciale : la part de la production vendue en bouteilles ;
# - la surface de l'exploitation en vigne, en hectare ; cette information est issue ici de la déclaration de récolte qui est effectuée chaque année ;
# - un indicateur du coût de la main-d'oeuvre à l'hectare dont il n'a pu être déterm in é jusqu'à présent qu'une prem ière approxim ation, m ais pour laquelle u n effort a été fait afin de prendre en compte la rém unération du personnel familial (les détails du calcul sont fournis en annexe). Son effet sur le coût est naturellem ent évident, au même titre que les coûts en équipements.
```

Les coûts des viticulteurs ... @laporte1996

Facteurs influençant le prix ... @outreville2010

Avant de conclure, nous proposons au lecteur une liste exhaustive des suppositions sur le comportement du marché des vins simples. 
Premièrement, nous supposons que chaque département à une fonction de production unique détérminée par des spécificités historiques, les traditions, la législation, le terroir, ainsi que des conditions météorologiques et géographiques.
Les effets sont fixes au niveau départamental et peuvent être isolés par des transformations spécifiques des données (ex : une transformation Within).
Deuxièmement, la quantité vendu sur le marché départamental est consommé au sein du même département. 
C'est une hypothèse très restrictive, qui nous eloigne de la réalité, mais nous devrions l'adopter si nous voulons intégrer les relations entre l'offre et la demande dans notre modèle. 
Afin de vérifier cette hypothèse nous allons construire deux modèles différents.
Finalement, les effets qu'on vise à estimer sont des effets moyens au niveau départamental.
C'est à dire nous allons obtenir un estimateur des effets moyens pour l'ensemble des département inclus dans notre analyse, ou des effets moyens au sein des groupes de département, si nous révèlons des differences significatives entre les départements. 
Un autre modèle nous permettra de vérifier et justifier cette hypothèse.

En ce qui concerne les pesticides, nous supposons d'abord, que l'utilisation des pesticides par les viticulteurs est relié à la demande sur le vin et les préférences des consomamteurs.
De plus, nous posons, que la demande des pesticides est inélastique au prix, ce qui nous permet d'exclure les interactions entre les fournisseurs des pesticides et les agriculteurs de notre analyse. 
La quantité de pesticides utilisés par les agriculteurs correspond seulement à leurs besoins. 

Pour résumer cette partie,  ce travail va porter sur les effets des pesticides sur l'offre des vins simples. 
Nous allons tester certaines hypothèses sur le comportement et l'organisation des relations sur le marché des vins simples en comparant les differents modèles. 
Puis, nous pourrons choisir entre ces modèles differents le plus vraisamblable, qui nous servira à répondre à la question de recherche. 

## Formalisation 

En formalisant notre modèle théorique de base, nous posons, que l'offre agregée pour toute la France est donnée identiquement par l'équation suivante : 

\begin{equation}
    Qo = \sum_{i = 1}^{N} qo_i
\end{equation}

Avec la quantité offerte déterminé par des contraintes de production et le prix sur le marché :

\begin{equation}
    qo_i = a_i + b_i Po_i + c_i X_i
\end{equation}

Où $X$ est un vecteur des variables explicatives influençant la production. Dans le cas le plus simple nous ne prenons en compte que les quantités des pesticides utilisées et la surface disponible, alors l'effet $c_{i1} : c_i = (c_{i1}, c{i2})$ represente l'effet d'utilisation des pesticides dans la production du vin sur l'offre de ce dernier.

Cette équation permet déjà d'estimer les effets de l'utilisation des pesticides sur le marché du vin. 
Appelons ce modèle théorique M1 pour le réfèrencer dans le futur, nous permettant de distinguer le cas sans intéractions simultanées entre l'offre et la demande. 

Il faut tenir compte que de cette façon nous ignorons plusieurs effets pervers, tels que :

- La structure du marché interne de la France ;
- La mobilité des produits finis entre des differents départements ;
- L'exportation et l'importation du vin.

Toutefois, ces résultats ne seront valables que dans la situation où la quantité de vin simple offerte sur le marché est déterminée seulement par le producteur et n'est pas lié à la demande. 
Comme nous l'avons vu dans la section précedente, la demande peut influencer les décisions des viticulteurs (ex: le choix de la procédure technique à suivre, d'utiliser ou non les pesticides, etc).
Dans ce cas, nous devrions prendre en compte les intéractions entre l'offre et la demande.
Dans ce but, nous introduisons également la demande dans notre analyse. 

La demande agregée du vin en France peut s'écrire sous la forme suivante :

\begin{equation*}
    Qd = \sum_{i = 1}^{N} qd_i 
\end{equation*}

Où $i \in \{1, ..., N\}$ sont des départements, chacun ayant sa propre fonction de demande unique : 

\begin{equation*}
    qd_i = \alpha_i + \beta_i Pd_i + \gamma_i Z_i 
\end{equation*}

Avec $Z$ étant l'ensemble des variables ayant une influence sur la demande du vin, dans le cas le plus simple nous n'utilisons que les revenus (c'est une des variables les plus utilisées dans des études empiriques sur le marché du vin).

Pour intégrer cette information dans notre *framework* analytique, nous devons construire un système d'équations.
Il existe plusieures façons de le faire. 

Dans le premier cas, nous pouvons essayer de capter les effets au niveau national.
Pour ce faire nous réécrivons les deux équation (de la demande et de l'offre respectivement) sous la forme suivante :

\begin{equation*}
    Q_o = \sum_{i = 1}^{N} (a_i + b_i Po_i + c_i X) = \sum_{i = 1}^{N} a_i + \sum_{i = 1}^{N} b_i Po_i + \sum_{i = 1}^{N} c_i X
\end{equation*}

\begin{equation*}
    Qd = \sum_{i = 1}^{N} ( \alpha_i + \beta_i Pd_i + \gamma_i Z_i ) = \sum_{i = 1}^{N} \alpha_i + \sum_{i = 1}^{N} \beta_i Pd_i + \sum_{i = 1}^{N} \gamma_i Z_i
\end{equation*}

Ce qui nous produira un système des deux équations, avec $Qd = Qo$ dans la situation d'équilibre :

\begin{align*}
    Qd & = \sum_{i = 1}^{N} \alpha_i + \sum_{i = 1}^{N} \beta_i Pd_i + \sum_{i = 1}^{N} \gamma_i Z_i \\
    Qo & = \sum_{i = 1}^{N} a_i + \sum_{i = 1}^{N} b_i Po_i + \sum_{i = 1}^{N} c_i X
\end{align*}

Neanmoins, ce cas se révèle être très complexe. 
D'abord, les effets peuvent être differents pour tous les départements, ce qui nous conduira à une augmentation dans le nombre des paramètres à estimer significative. 
De plus, même si tous les effets sont identiques pour l'ensemble des départements, des contraintes au niveau des données peuvent se révèler trop restrictives, réduisant, ainsi à néant la puissance statistique de notre estimateur (ex : le nombre des observations par années très faible). 
Dans le deux cas nous faisons face à une impasse.

Une des modifications possibles dans ce cas sera l'introduction d'une contrainte supplémentaire au niveau de la demande sur le vin de table.
Afin de pouvoir identifier les effets de toutes les variables par un système d'équations, nous pouvons supposer, que tout le vin produit dans un département est consommé dans le même department. 
Dans ce cas nous pourrions obtenir des estimateurrs pour les effets moyens au niveau départemental.
Toutefois, c'est une supposition forte qui nous éloigne de la réalité. 

Théoriquement, nous pouvons tout de méme ignorer ces effets, car nous visons à estimer les effets moyens pour tous les départements. 
De cette façon, lors de l'agrégation des effets au niveau national en estimant le coefficient moyen unique pour tous les départements nous allons réduire les biais possibles.

Alors,nous pouvons réécrire notre système d'equations sous la forme suivante :

\begin{align*}
  qd_i & = \alpha_{i} + \beta Pd_{i,d} + \gamma Z_{i} \\
  qo_i & = a_i + b Po_{i,o} + c X_{i} \\ 
\end{align*}

Où $qd_i = qo_i$ et $Pd_i = Po_i$, ce qui permet de relier les équations au niveau départemental.
Les coefficients $b$, $c$, $\beta$ et $\gamma$ sont supposés fixes pour tous les départements. Ils nous donnent un estimateur des effets moyens au niveau de la France.
L'effet des pesticides dans la production du vin sera capté par le terme $c_{1} : c = (c_{1}, c{2})$ dans ce cas.

Néanmoins, nous nous posons la question, comment réagir dans le cas où les effets sont differents pour les differents départements à cause des spécificité des marché locaux, géographiques ou autres ?
On peut supposer, qu'il existe au moins quelques groupes majeures ayant des caractéristiques et des comportements similaires. 
Dans ce cas nous pourrions construire des clusters, qui regrouppent des départements ayant des caractéristiques identiques. 
Cela nous permettra de modèliser les effets moyens par cluster en réduisant les biais eventuels.

Ce système peut être formalisé par les $K$ systèmes d'équations suivants :

\begin{align*}
  qd_{i_{c = const}} & = \alpha_{i_{c = const}} + \beta_{c = const} Pd_{i_{c = const},d} + \gamma_{c = const} Z_{i_{c = const}} \\
  qo_{i_{c = const}} & = a_{i_{c = const}} + b_{c = const} Po_{i_{c = const},o} + c_{c = const} X_{i_{c = const}} \\ 
\end{align*}

Où $c$ décrit l'appartenance des départements à un des groupes (clusters).

# 4. Les données

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Données mob  ###################
#####################################################
```

Avant de passer à la discussion des modèles économétriques il nous faut prendre connaissance de la nature des données en notre disposition.
Dans cette partie de notre travail nous allons presenter la base des données utilisé lors de cette étude. 
Nous commencerons par une presentation des sources et des types des données extraits de ces sources. 
Puis, nous procederons avec la déscription des méthodes et thécniques utilisées pour transformer ces données et les rendre traitables. 
Finalement, nous presenterons un dictionnaire des variables pour nos bases des données.

## Sources des données : 

Nous avons utilisé les bases des données suivantes pour notre analyse :

- Les données de ventes de pesticides par département (INERIS)
- Les données sur les prix du vin (France Agrimer)
- Les données sur la population (INSEE)
- Les données sur la production de vin (SSM Finances Publiques)

## Les variables utilisées pour notre modèle

\noindent\rule[0.5ex]{\linewidth}{1pt}

\textcolor{red}{Réverifier tous les sources et la naure des données ...}

\textcolor{red}{Expliciter la procedure de création des variables}

\textcolor{red}{Preciser les effets attendus des variables}

\textcolor{red}{Discuter les externalités (ou c'est mieux de l'inclure dans la partie théorique ? ou contextualisation ? A VOIR)}

\noindent\rule[0.5ex]{\linewidth}{1pt}

Dans notre étude nous faisons face à un problème avec deux variables endogènes et trois variables exogènes.

Variables endogènes : 
- la quantité totale produite de vin rouge et blanc non IG par département (en hectolitres, en log), 
- le prix moyen des vins rouges-blancs (idice, en log).

Variables exogènes : 
- le revenu médian par département (en euros par personne par année, en log), 
- la surface agricole destinée aux vins de table (en hectares, en log),
- la quantité des pesticides utilisés sur la vigne (indice, en log).

Au niveau des pesticides, on va s’intéresser plus particulièrement aux quantités de produits vendus par département entre 2009 et 2017 utilisés principalement sur les cultures viticoles. 
Il faut faire preuve de vigilance sur le conditionnement des produits qui n’est pas exprimé dans la même unité au sein de cette base : en litres ou en kilos.
Dans notre étude nous allons étudier l'impact de la masse totale des pésticides utilisés.
Pour pouvoir le faire, nous créons un indice qui permet de prendre en compte les évolutions des differents types des produits à la fois.
Nous créons un indice simple :

\begin{equation*}
  P = \frac{\sum_j p_{j, t} q_{j, t}}{\sum_j p_{j, 0} q_{j, 0}}
\end{equation*}

Avec $j$ désignant le produit $j$, et $p$ étant un coefficient de pondération (dans le cas le plus simple $p = 1$).

En ce qui concerne les données sur le prix du vin, on s’intéresse principalement au prix moyen des vins rouge- rosés et blancs sans IG (Indication Géographique) sur la période 2009-2017. 
Ces prix sont déflatés par l’indice des prix à la consommation (base 100 en 2014). 
On ne considère ici que le prix moyen déflaté au niveau national.
Dans le deuxième modèle nous avons besoin de créer artificiellement un estimateur qui va varier par département.
Dans ce but nous créons l'indice de prix du vin de table départementale, calculé de façon suivante :

\begin{equation*}
  P = \frac{p_{rouge, t} q_{rouge, t} + p_{blanc, t} q_{blanc, t}}{p_{rouge, 0} q_{rouge, 0} + p_{blanc, 0} q_{blanc, 0}}
\end{equation*}

Avec $t$ étant l'anée au période $t$.

Au niveau des données sur la population, la variable qui nous intéresse ici est relative au niveau de revenu, exprimée au niveau départemental (laquelle, si besoin nous pourrions facilement aggréger au niceau national). 
Plus précisément, on va utiliser le revenu médian par département.
Il est aussi déflatée de l’indice des prix à la consommation (base 100 en 2014).

Toutes les variables subissent une transformation logarithmique, ce qui nous permet d'interpreter les effets estimés plus facilement. 
Pour un modèle logarithmique nous pourrions traiter les estimateurs obtenus comme l'elasticité de la demande/l'offre par rapport à des facteurs differents. 
Ainsi, nous cherchons particulièrement l'élasticité des quantités offertes sur le marché par rapport à la quantité des pesticides utilisés.

Les propriétés de ces données sont les suivantes :

- Toutes les variables varient par département et par année.
- Le période temporelle comprise dans notre échantillon est de 2012 à 2016.
- Nous ne considérons que les régions produisant du vin. 
- Nous éliminons les effets fixes pour en substrayant les moyennes départamentales.
- Données en panel "cylindrées".
- Nombre des individus large (69 départements, qui produisent le vin simple et qui utilisent des pesticides) et le nombre des périodes pauvre (5 périodes).

# 5. L'étude statistique

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Statistique  ###################
#####################################################
```

Dans cette partie de l'étude nous allons mener une étude exploratoire sur les données collectées. 

De l'étude de la variance pour les données en panel avec des statistiques générales, nous passerons à l'étude de l'interdependance des variables. 
Puis, nous allons finir avec l'étude des données alternées par une transformation **within**.

## Visualisation au niveau de la France 

Pour la première analyse il peut être interessant de voir la situation du point du vue géographique. 
Nous visualisons les valeurs moyennes par département des differentes variables (une partie des répresentation se trouve dans l'annexe X).

```{r include = FALSE}
# Creation de la base des données 
SpData = datai %>% 
    group_by(ndep) %>%
    summarise_all(mean) %>%
    dplyr::select(ndep, qi, ipi, iki, ri, si)
# Formes
formes = getData(name = "GADM", country = "FRA", level = 2)
# plot(formes, main = "Carte de la France, départements")
# Index 
ind = match(as.numeric(formes$CC_2), SpData$ndep)
# Data transfer
qisp = SpData[ind, "qi"]
ipisp = SpData[ind, "ipi"]
ikisp = SpData[ind, "iki"]
sisp = SpData[ind, "si"]
risp = SpData[ind, "ri"]
# Writting 
formes$qi = qisp
formes$ipi = ipisp 
formes$iki = ikisp 
formes$ri = risp 
formes$si = sisp 
```

D'abord nous étudions le comportement de la variable dépendante de notre système. 
La quantité de vin sans IG produit par département semble pouvoir être correlée à partir de la figure suivante.

\FloatBarrier

```{r echo = FALSE, results = "asis", fig.cap = "Les quantité du vin non-IG moyennes par département"}
# Plot 
spplot(formes, "qi", 
    col.regions = colorRampPalette(c('grey96', 'red'))(30),  
    main = list(label = "Quantité du vin produite par département", cex = 0.8))
```

\FloatBarrier

Puis, nous observons le comportement du reste des variables (les représentations graphiques sont groupés dans l'annexe X).
L'indice des prix se comporte pratiquement comme la quantité du vin produite, car cet indice fut construit par l'intermédiaire de cette variable. 
Les autres moyennes ne semblent pas avoir des structures corrélées dans l'espace au niveau de la France. 
Dans notre analyse nous nous laissons la liberté d'ignorer les effets possibles d'autocorrélation spatiale dans nos données. En effet, au moment de la constructions de notre base de données, nous avons ignoré les départements ne produisant pas de vin simple. Mais, ils peuvent quand même jouer un rôle si nous prenions en compte la structure spatiale de nos données. 

## Etude de la variance 

Passons maintenant, à l'étude de la variance. 
Nous allons décortiquer la variance par type (between et within) afin d'obtenir une idée sur le choix préférable de la dimension d'agrégation de nos données, car il se peut que la théorie ne corresponde pas à la réalitée (ex: nous faisons face aux effets fixes par année et non par département).

```{r include = FALSE}
lxtsums = list()
# list
lxtsums[[1]] = xtsum(datai, ipi, ndep)
lxtsums[[2]] = xtsum(datai, iki, ndep)
lxtsums[[3]] = xtsum(datai, si, ndep)
lxtsums[[4]] = xtsum(datai, ri, ndep)
lxtsums[[5]] = xtsum(datai, t, ndep)
# results
results = print.xtsum(lxtsums)
rownames(results) = c("Index prix", "Index pesticides",
    "Surface", "Revenus", "Temps")
```

Le tableau suivant regroupe les statistiques déscriptives essentielles : 

- Moyennes 
- Variance sur l'échantillon complet 
- Variance *between* 
- Variance *within*

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(results,  
    header = FALSE,
    title = "Etude de la variance",
    summary = FALSE)
```

\FloatBarrier

Il est facile de remarquer que la variance *between* est plus significative que la variance *within*. 
Cela nous amène à l'idée qu'il faut utiliser un modèle qui permettra d'estimer et de corriger ces inégalités entre les individus, car nous sommes plus interessés par des effets individuels moyens (les effets moyens pour tous les départements).
Cela est complètement conforme à l'hypothèse que l'on a exprimé lors de la formalisation du modèle économique théorique.

```{r include = FALSE}
Formulas = list(
    ipi = qi ~ ipi,
    iki = qi ~ iki,
    si = qi ~ si,
    ri = qi ~ ri)
Dtest = Effect.testing(Formulas, data = datap)
rownames(Dtest) = c("Index prix", "Index pesticides",
    "Surface", "Revenus")
```

De plus, il est intéressant d'observer les résultats obtenus pour le test de Chow comparant le modèle complet (*pooled model*) contre les modèles aux effets fixes et aléatoires. 
Le tableau suivant regroupe les p-valeurs de ce test pour les différents modèles univariées.

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(Dtest[,c(1:2)],  
    header = FALSE,
    title = "Les p-valeurs de pooling-test de Chow",
    summary = FALSE)
```

\FloatBarrier

A part le cas de la surface nous ne pouvons pas rejeter l'hypothèse nulle, spécifiant que les individus ont des effets identiques pour toute la population. 

## L'étude des types d'effets  

Nous avons déjà vu, qu'il est fortement probable que nous faisions face à un modèle à effets fixes individuelles. 
Il faut quand même le justifier.
Pour faire cela, nous allons effectuer le test du multiplicateur de Lagrange sur la nature des effets (individuels, temporels ou en double dimention). 
Selon les résultats des tests il est difficile de choisir arbitrairement un type d'effets. 
Il est évident que nous avons des effets fixes au niveau individuel ou des effets fixes en double dimension pour toutes les variables. 

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(Dtest[,c(3:ncol(Dtest))], 
    header = FALSE,
    title = "p-valeurs de Lagrange multiplier test",
    summary = FALSE)
```

\FloatBarrier

Selon les résultats obtenus, ainsi que les évidences théoriques des études antérieurs nous décidons de ne garder que les effets fixes au niveau individuel afin de faciliter l'analyse.

## L'analyse de la corrélation

Dans le tableau ci-dessous nous présentons les corrélations des variables après la correction pour les effets fixes individuels (nous effectuons la transformation *within* sur nos données en soustrayant les moyennes individuelles pour l'ensemble des variables).
Dans les annexes nous proposons également un tableau de corrélation pour les données non-transformées, ce qui permet d'observer les inégalités et une pauvre répresentativitée des liens entres les variables pour les données initiales.

```{r include = FALSE}
###################
# Rework the matrix WITHIN_TRANSFORM
###################
rm(datax) ; rm(datay) ; rm(data) ; rm(dn)
dataW = datap 
dataW$qi = Within(datap$qi)
dataW$ipi = Within(datap$ipi)
dataW$iki = Within(datap$iki)
dataW$si = Within(datap$si)
dataW$ri = Within(datap$ri)
```

Particulierement nous pouvons remarquer une forte corrélation entre la quantité offerte et le prix d'équilibre.
Egalement ...

# 6. Modèlisation

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Modèlisation  ##################
#####################################################
```

\noindent\rule[0.5ex]{\linewidth}{1pt} 

\textcolor{red}{Séparer les modèles (OLS, 3SLS avec justification par 2SLS et la comparaison avec i3SLS, clusters en OLS et 3SLS).}

\textcolor{red}{Justifier le choix des modèles par 3 cas théoriques. Discuter les avantages et les inconveniences}

\textcolor{red}{Ajouter des liens avec des études méthodologiques precedents.}

\textcolor{red}{Pour le modèle 2SLS préciser la forme, tester les instruments}

\textcolor{red}{Arbitrage du choix de 2SLS vs 3SLS}

\noindent\rule[0.5ex]{\linewidth}{1pt}

Cette partie du travail abordera la formulation économétrique de notre problème.
Nous allons débuter par la présentation des notions théoriques utilisées dans ce travail, suivis par la formalisation économétrique du modèle théorique que nous avons spécifié dans la séction 5.
Après, nous expliquerons la stratégie d'identification utilisée.

## Presentation de la méthodologie

L'AIDS (*almost ideal demand system*) et les autres modèles de demande cités dans la littérature ont de nombreuses lacunes qui les rendent impropres pour l'estimation du marché du vin, selon @cembalo2014. 
Dans notre étude nous allons, tout de même, utiliser une approche similaire à ce modèle là, sous des suppositions restrictives. 

Ce modèle nous permettra de simuler l'équilibre sur le marché du vin, prenant ainsi en compte la plupart des facteurs incitant les producteurs de vin à utiliser les pesticides. 

## Modèle économétrique 

Dans cette section, nous allons présenter un par un nos modèles économétriques correspondant chacun à un des trois cadres théoriques possibles.
Tous les modèles visent à estimer les effets moyens pour tous les départements sous des hypothèses differentes de fonctionnement de marché.
Dans tous les cas, l'agrégation des effets au niveau national (ou au niveau des groupes) nous permet de réduire les biais eventuels, liés à la mauvaise spécification du modèle.

Pour le cadre où nous n'observons pas les intéractions entre la demande et l'offre sur le marché (M1). Nous estimons un modèle simple.
Nous écrivons notre modèle sous la forme suivante :

\begin{equation*}
  qo_{i,t} = a_1 + b Po_{i,t} + c X_{i,t} + u_{i,t}
\end{equation*}

A ce point nous avons un choix : soit nous supposons que les agriculteurs sont des preneurs de prix, ce qui nous permet de traiter le prix comme une variable exogène; soit nous devrions construire un estimateur de variables instrumentales afin de traiter l'endogénéité eventuelle de l'indice des prix.
Evidement le premier cas est le plus simple, mais pour justifier l'utilisation de cette méthode nous devrions effectuer des tests d'énogénéité de prix. 
Le deuxième cas est beaucoup plus réaliste, puisque les viticulteurs sont rarement preneurs de prix et l'offre aussi joue son rôle sur l'équilibre du marché.

Dans la dernière situation nous utilisons les idées de @mackay2018, supposant que les variables déterminant la demande sont des instruments fiables pour la prédiction des variables endogènes dans l'équation d'offre (bien que dans notre cas nous ignorons les effets des intéractions entre l'offre et la demande).
Particulièremnt ici, nous pourrions utiliser les données sur les revenus afin d'instrumenter le niveau des prix (l'indice des prix du vin).

Passons maintenant au modèle plus complexe (M2), basé sur l'hypothèse que la demande influence l'offre, affectant également le l'utilisation des pesticides par les agriculteurs.
Nous pouvons réécrire notre système d'equations dans ce cas sous la forme suivante :

\begin{align*}
  qo_{i,t} & = a_1 + b Po_{i,t} + c X_{i,t} + u_{i,t} \\ 
  qd_{i,t} & = \alpha_{i} + \beta Pd_{i,t} + \gamma Z_{i,t} + \epsilon_{i,t}  \\
\end{align*}

Nous posons que l'offre et la demande sont egaux au niveau de chaque département : $qd_{i,t} = qo_{i,t}$.
C'est à dire que l'offre interne du département vise à satisfaire la demande interne du même département. 

En termes d'agrégation ex-post des effets estimés, nous sommes sensés tomber sur l'équilibre au niveau du marché national. 
En d'autre mots, le système (qui implique : $Qd = Qo$) :

\begin{equation*}
  qd_{i,t} = qo_{i,t}
\end{equation*}

Au point d'équilibre nous rencontrons également l'égalité des prix :

\begin{equation*}
  Po_{1,t} = Pd_{1,t}
\end{equation*}

De cette façon nous obtenons un système des systèmes des équations.
En simplifiant l'écriture nous pouvons la représenter sous la forme suivante :

\begin{align*}
  q_{i,t} & = \alpha_{i} + \beta P_{i,t} + \gamma Z_{i,t} + \epsilon_{i,t} \\
  q_{i,t} & = a_i + b P_{i,t} + c X_{i,t} + u_{i,t}
\end{align*}

Et finalement, nous pouvons estimer les deux modèles (M1 et M2) en regroupant les département par leurs caractéristiques.
Appelons ces modèles M3.1 et M3.2 respectivement.

Le premier prenant la forme :

\begin{align*}
  qo_{i,t} & = a_1 + b Po_{i,t} + c X_{i,t} + u_{i,t} \\ 
\end{align*}

Tandis que le dernier :

\begin{align*}
  q_{i_{c},t} & = \alpha_{i_{c}} + \beta P_{i_{c},t} + \gamma Z_{i_{c},t} + \epsilon_{i_{c},t} \\
  q_{i_{c},t} & = a_i + b P_{i_{c},t} + c X_{i_{c},t} + u_{i_{c},t}
\end{align*}

Avec $c$ décrivant l'appartenance du département à un des clusters.

Pour finir cette partie, nous avons à notre disposition plusieurs chemins differents pour traiter ce modèle du point de vue économétrique. 
Le plus simple est d'estimer l'effet des pesticides sur l'offre de vin en ignorant les impacts du comportement des consommaterus sur les producteurs. 
Cette méthode implique une estimation par OLS simples (ou IV-OLS, lesquels introduisent la notion d'endogénéité des prix).
De l'autre coté, nous pouvons utiliser les triples moindre carrés (nous devrions comparer les résultats obtenus avec un système d' équations non-réliées, éstimé par 2SLS afin de traiter l'endogenèité), qui nous permettront d'obtenir des résultats identiques aux résultats d'estimations des équations structurelles sous l'hypothèse de l'intéraction entre l'offre et la demande. 
Cette méthode offre la possibilité d'estimer le système d'équations avec plusieurs variables endogènes en prenant en compte les deux coté du marché, à la fois. 
Finalement, si on trouve qu'il existe une heterogeneité entre les départements en termes d'équilibre interne, nous pourrions réestimer les modèles en clusterisant nos *individus* (départements) par des classes différentes selon leurs attributs, pour estimer les equations par cluster.

```{r include = FALSE, eval = FALSE}
###################
# Not evaluated !!!
###################
###################
# Rework the matrix DUMMIES
###################
Dum = dummy(datai$ndep, sep = "_")
# Index pésticides
IKI = as.matrix(datai$iki)[, rep(1, each = length(unique(datai$ndep)))]
ikiDum = as.data.frame(Dum*IKI) %>% 
    setNames(paste0('iki_', names(.)))
rm(IKI)
# Revenus
RI = as.matrix(datai$ri)[, rep(1, each = length(unique(datai$ndep)))]
riDum = as.data.frame(Dum*RI) %>% 
    setNames(paste0('ri_', names(.)))
rm(RI)
# Concatenate
dataD = datai %>% 
    dplyr::select(-c(t, ri, iki)) %>% 
    cbind(Dum) %>% 
    cbind(ikiDum) %>% 
    cbind(riDum)
```

```{r include = FALSE, eval = FALSE}
###################
# Not evaluated !!!
###################
###################
# Rework the matrix WITHINxDUMMIES
###################
Dum = dummy(datai$ndep, sep = "_")
# Index pésticides
IKI = as.data.frame(dataW$iki, ncol = 1)[, rep(1, each = length(unique(datai$ndep)))]
ikiDum = as.data.frame(Dum*IKI)
ikiDum = ikiDum %>%
    set_names(~str_replace_all(., "dataW\\$", ""))
rm(IKI)
# Revenus
RI = as.data.frame(dataW$ri)[, rep(1, each = length(unique(datai$ndep)))]
riDum = as.data.frame(Dum*RI) %>%
    rename_all(list(~str_replace_all(., "dataW\\$", "")))
rm(RI)
# Concatenate
dataWD = dataW %>% 
    dplyr::select(-c(t, ri, iki)) %>% 
    cbind(ikiDum) %>% 
    cbind(riDum)
rm(ikiDum) ; rm(riDum)
```

## Hypothèses sur les résultats 

Nous attendons que l'estimateur de 3SLS, qui permet de capter les effets de corrélations entre les équations en présence de plusieures variables exogènes nous permettra d'obtenir des estimations plus fiables. 
Cette méthode nous permet à depasser le biais de simultanéité qui apparaît dans le cas d'estimation des systèmes d'équations liés (dans notre cas nous étudions les effets des pésticides sur l'offre et production du vin simple sous l'hypothèse de présence des effets du marché).
L'estimateur pareil donne des résultats similaires à l'estimateur de ILS (*indirect least squares*).
De plus, sa version iterée (qui converge à des résultats similaires à ceux obtenus par l'éstimation avec maximum de vraisamblance) donne des résultats avec la variance la plus faible.

Les propriétés de cet estimateur sont :

- Consistence ;
- Efficience (asymptotique) ;
- La distributions pour les estimateurs suit une loi normale seulement dans des grands échantillons.

Dès le debut nous envisagions que cet estimateur ne reflètera pas la nature du marché. 
C'est pourquoi nous, dans ce travail, testons plusieurs modèles.

Parmi les inconvenients éventuels, on a également la faible représentation des effets hetérogènes entre les départements par le modèle. 
Nous estimons seulement les effets moyens et ainsi nous ignorons les différences des élasticités pour des départements différents. 
Hereusement ce problème peut être rémédié par l'introduction des clusters, regroupant des départements ayant des comportements similaires.

Finalement, il existe des effets que l'on ignore complètement, mais qui risquent d'intervenir. 
Par exemple, nous ignorons la présence d'autocorrélation spatiale et/ou temporelle dans notre modèle. 
Egalement, un nombre probablement insuffisant de facteurs est utilisé dans ce modèle, ce qui augmente le risque du biais des variables omises dans nos estimations. 

# 7. Résultats des estimations 

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Modèlisation  ##################
#####################################################
```

Dans cette section nous allons présenter les résultats économétriques pour les différents modèles et les comparer.

```{r eval = FALSE, include = FALSE}
# Nous allons presenter en particulier :

# - Les coefficients estimés avec leurs variance 
# - L'efficience et comparaison des estimateurs 
# - Etude des erreurs 
#     - La distribution des erreurs
#         - La normalité 
#         - Centrage sur 0 
#         - Independance des variables explicatives 

#     - L'autocorrelation des résidus 
#     - L'hétéroskedacité  
```

Nous estimons un ensemble de modèles différents possibles afin de pouvoir choisir la méthode la plus raisonnable. 
Les modèles suivants sont traités séparement :

- M1 : modèle simple sans intéraction entre l'offre et la demande ;
- M2 : modèle complexe visant à intégrer les intéractions entre l'offre et la demande en présence de variables éndogènes ;
- M3 : les modèles sur les données clustérisées (M3.1 et M3.2 respectivement pour les deux cas précédents).

```{r include = FALSE, eval = FALSE}
# - Vérification des hypothèses (5 hypothèses) :
#     - La moyenne nulle des erreurs 
#     - La normalité des residus 
#     - Homoscedacité 
#     - Autocorrélation 
#     - Spécification du modèle
# 3SLS and FIML are asymptotically equivalent. 
# Hence 3SLS is efficient and FIML is consistent even if residuals are not normal.
```

## Les résultats en absence d'intéractions

```{r include = FALSE}
# Data transformation for systemfit
dataWX = as.data.frame(dataW)
```

```{r include = FALSE}
# equations
eqdemand = qi ~ 0 + ipi + ri
eqoffer = qi ~ 0 + ipi + si + iki 
system = list(Demande = eqdemand, Offre = eqoffer)
# OLS
ols = systemfit(system, 
    data = dataWX, 
    method = "OLS")
# WLS
wls = systemfit(system, 
    data = dataWX, 
    method = "WLS")
# SUR
sur = systemfit(system, 
    data = dataWX, 
    method = "SUR")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
texreg(float.pos = "!htbp", 
    list(ols, wls, sur),
    custom.model.names = c("OLS", "WLS", "SUR"),
    label = "table : ols, wls and sur")
```

\FloatBarrier

## Les résultats 2SLS, W2SLS, 3SLS et i3SLS

\FloatBarrier

```{r include = FALSE}
# equations
eqdemand = qi ~ 0 + ipi + ri
eqoffer = qi ~ 0 + ipi + si + iki 
inst = ~ ri + si + iki
system = list(Demande = eqdemand, Offre = eqoffer)
# 2SLS
# 2SLS is an equivalent of ILS (indirect least squares)
sls2 = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "2SLS")
# 2WSLS
wsls2 = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "W2SLS")
# 3SLS (errors correction)
sls3 = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "3SLS")
# FIML (iterated 3SLS)
fiml = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "3SLS", maxit = 1000)
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
texreg(float.pos = "!htbp", 
    list(sls2, wsls2, sls3, fiml),
    custom.model.names = c("2SLS", "W2SLS", "3SLS", "i3SLS"),
    label = "table : 2sls, w2sls, 3sls and fiml")
```

\FloatBarrier

## Clusterisation et modèlisation par groupe 

### *Between* 

Nous avons vus dans le comportement des résidus une nature non-aléatoire groupé. 
Cela nous amène à l'idée de construire k-clusters pour modèliser les relations par groupe.

Nous supposons que les départements ayant des valeurs moyennes interannuelles proches (transformation Between) ont un comportement identique.
La clusterisation est effectuée sur les données Between pour les départements.

```{r include = FALSE}
# Clustering 
# Between dataframe
dataB = datap 
dataB$qi = Between(datap$qi)
dataB$ipi = Between(datap$ipi)
dataB$iki = Between(datap$iki)
dataB$si = Between(datap$si)
dataB$ri = Between(datap$ri)
dataB$ndep = index(datap)$ndep
# Between correction
dataB = dataB %>% 
    dplyr::select(-t)
dataB = dataB %>% 
    group_by(ndep) %>% 
    summarise_all(mean)
# Analysis for clustering
wss = (nrow(dataB[,-1])-1)*sum(apply(dataB[,-1], 2, var))
for (i in 2:15) {
    wss[i] = sum(kmeans(iter.max = 100, dataB[,-1], centers = i)$withinss)
}
```

Nous pouvons supposer que le nombre des clusters optimaux est entre 3 et 5.
Prenant en compte les graphiques des résidus vus lors des analyses des modèles, nous allons supposer qu'il n'y a que 3 clusters principaux.

\FloatBarrier

```{r echo = FALSE, fig.cap = "Le choix des clusters"}
plot(1:15, wss, type = "l", 
    xlab = "Nomber de clusters",
    ylab = "WSS") # 3, 4, 5
points(1:15, wss, col = "red")
```

\FloatBarrier

```{r include = FALSE}
dataWi = as.data.frame(dataW) 
dataWi$ndep = as.factor(index(dataW)$ndep)
```

```{r include = FALSE}
# Grouping
fit1 = kmeans(iter.max = 100, dataB[,-1], 3)
# Sup 

nclef = data.frame(ndep = dataB$ndep, clust = fit1$cluster)
dataWY = left_join(dataWi, nclef, by = "ndep")
```

### *Within*

Nous avons vus dans le comportement des résidus une nature non-aléatoire groupé. 
Cela nous amène à l'idée de construire k-clusters pour modèliser les relations par groupe.

D'abord on compare le comportement des cluster pour les données à l'information complète et les données Within.

Comme nous pouvons le voir dans les résultats le nombre de clusters optimaux est trop large pour les séparer dans l'analyse :

```{r include = FALSE}
# Clustering 
# Data creation
dataClust = dataWi
dataClust_t1 = dataClust %>% 
    filter(t == 1) %>% 
    dplyr::select(-t)
dataClust_t2 = dataClust %>% 
    filter(t == 2) %>% 
    dplyr::select(-t)
dataClust_t3 = dataClust %>% 
    filter(t == 3) %>% 
    dplyr::select(-t)
dataClust_t4 = dataClust %>% 
    filter(t == 4) %>% 
    dplyr::select(-t)
dataClust_t5 = dataClust %>% 
    filter(t == 5) %>% 
    dplyr::select(-t)
```

```{r include = FALSE}
# Joining data
dataC = inner_join(dataClust_t1, dataClust_t2, 
    by = "ndep", suffix = c("", ".t2")) 
dataC = inner_join(dataC, dataClust_t3, 
    by = "ndep", suffix = c("", ".t3")) 
dataC = inner_join(dataC, dataClust_t4, 
    by = "ndep", suffix = c("", ".t4")) 
dataC = inner_join(dataC, dataClust_t5, 
    by = "ndep", suffix = c("", ".t5")) 
```

```{r include = FALSE}
# Analysis for clustering
wss = (nrow(dataC[,-6])-1)*sum(apply(dataC[,-6], 2, var))
for (i in 2:15) {
    wss[i] = sum(kmeans(iter.max = 100, dataC[,-6], centers = i)$withinss)
}
```

Nous pouvons supposer que le nombre des clusters optimaux est entre 6 et 15.

\FloatBarrier

```{r echo = FALSE, results = "asis"}
plot(1:15, wss, type = "l", 
    xlab = "Nomber de clusters",
    ylab = "WSS") # 3, 4, 5
points(1:15, wss, col = "red")
```

\FloatBarrier

```{r include = FALSE}
# Grouping
fit2 = kmeans(iter.max = 100, dataC[,-6], 6)
# require(FactoMineR)
nclef2 = data.frame(ndep = as.factor(dataC$ndep), clust = fit2$cluster)
dataWZ = left_join(dataWi, nclef2, by = "ndep")
clusters2 = cbind(fit2$centers, fit2$size, k <- c(1:6)) %>%
    as.data.frame()
```

### Information complète

Dans le cas de l'information complète, on a :

```{r include = FALSE}
# Clustering 
# Data creation
dataClustx = datai 
dataClustx_t1 = dataClustx %>% 
    filter(t == 1) %>% 
    dplyr::select(-t, -year)
dataClustx_t2 = dataClustx %>% 
    filter(t == 2) %>% 
    dplyr::select(-t, -year)
dataClustx_t3 = dataClustx %>% 
    filter(t == 3) %>% 
    dplyr::select(-t, -year)
dataClustx_t4 = dataClustx %>% 
    filter(t == 4) %>% 
    dplyr::select(-t, -year)
dataClustx_t5 = dataClustx %>% 
    filter(t == 5) %>% 
    dplyr::select(-t, -year)
```

```{r include = FALSE}
# Joining data
dataCx = inner_join(dataClustx_t1, dataClustx_t2, 
    by = "ndep", suffix = c("", ".t2")) 
dataCx = inner_join(dataCx, dataClustx_t3, 
    by = "ndep", suffix = c("", ".t3")) 
dataCx = inner_join(dataCx, dataClustx_t4, 
    by = "ndep", suffix = c("", ".t4")) 
dataCx = inner_join(dataCx, dataClustx_t5, 
    by = "ndep", suffix = c("", ".t5")) 
```

```{r include = FALSE}
# Analysis for clustering
wss = (nrow(dataCx[,-1])-1)*sum(apply(dataCx[,-1], 2, var))
for (i in 2:15) {
    wss[i] = sum(kmeans(iter.max = 100, dataCx[,-1], centers = i)$withinss)
}
```

Nous pouvons supposer que le nombre de cluster optimal est entre 3 et 5.
Prenant en compte les graphiques des résidus vus lors de l'analyse des modèles, nous allons supposer qu'il n'y a que 3 clusters principaux.

\FloatBarrier

```{r echo = FALSE, results = "asis"}
plot(1:15, wss, type = "l", 
    xlab = "Nomber de clusters",
    ylab = "WSS") # 3, 4, 5
points(1:15, wss, col = "red")
```

\FloatBarrier

```{r include = FALSE}
# Grouping
fit3 = kmeans(iter.max = 100, dataC[,-6], 3)
# require(FactoMineR)
nclef3 = data.frame(ndep = as.factor(dataC$ndep), clust = fit3$cluster)
dataWZ = left_join(dataWi, nclef3, by = "ndep")
clusters3 = cbind(fit3$centers, fit3$size, k <- c(1:3)) %>%
    as.data.frame()
```

### Modèlisation 

\FloatBarrier

```{r include = FALSE}
# Data transformation
dataWZ = dataWZ %>% 
    mutate(ipi1 = ipi*as.numeric(clust == 1),
        ipi2 = ipi*as.numeric(clust == 2),
        ipi3 = ipi*as.numeric(clust == 3),
        ri1 = ri*as.numeric(clust == 1),
        ri2 = ri*as.numeric(clust == 2),
        ri3 = ri*as.numeric(clust == 3),
        si1 = si*as.numeric(clust == 1),
        si2 = si*as.numeric(clust == 2),
        si3 = si*as.numeric(clust == 3),
        iki1 = iki*as.numeric(clust == 1),
        iki2 = iki*as.numeric(clust == 2),
        iki3 = iki*as.numeric(clust == 3))
```

```{r include = FALSE}
# Equations
eqdemandz = qi ~ 0 + ipi +
    ri1 + ri2 + ri3 
eqofferz = qi ~ 0 + ipi + 
    si1 + si2 + si3 +
    iki1 + iki2 + iki3
instz = ~ ri1 + ri2 + ri3 + 
    si1 + si2 + si3 +
    iki1 + iki2 + iki3
systemz = list(Demande = eqdemandz, Offre = eqofferz)
```

Nous évaluons le système en introduisant les variables de groupe (dummy variables) sous l'hypothèse des résidus joints.

```{r include = FALSE}
# OLS
olsx = systemfit(systemz, 
    data = dataWZ, 
    method = "OLS")
# 2SLS
sls2x = systemfit(systemz, 
    inst = instz,
    data = dataWZ, 
    method = "2SLS")
# 3SLS
sls3x = systemfit(systemz, 
    inst = instz,
    data = dataWZ, 
    method = "3SLS")
```

Les résultats obtenus sont les suivants : 

\FloatBarrier

```{r echo = FALSE, results = "asis"}
texreg(float.pos = "!htbp", 
    list(olsx, sls2x, sls3x),
    custom.model.names = c("OLS", "2SLS", "3SLS"),
    label = "table : ols, 2sls et 3sls, full information clusters")
```

\FloatBarrier

# 9. Conclusions 

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Modèlisation  ##################
#####################################################
```

- Le marché du vin
- Le rôle des pésticides  
- Validité 

\FloatBarrier

## Le marché du vin
- Un comportement inattendus 
    - Les effets de substitution contre les produits de la haute gamme 
    - Les effets négatives du revenu 
    - 

\FloatBarrier

## Le rôle des pésticides

- Confirmation des résultats des études précedentes 
    - Utilisés pour réduire les pertes 

\FloatBarrier

## Validité 

- Faible validité du modèle économétrique 
    - Variables ommises 

\FloatBarrier

\newpage

# Annexes 

```{r eval = FALSE, include = FALSE}
################################################
###################  Annexes  ##################
################################################
```

## A Les statistiques déscriptives 

### A1 Les moyennes par département 

\FloatBarrier

```{r echo = FALSE, results = "asis", fig.cap = "Les valeurs moyennes par département, partie 1"}
# Plot
p1 = spplot(formes, "ipi", 
    col.regions = colorRampPalette(c('grey96', 'blue'))(30),  
    main = list(label = "Index prix du vin par département", cex = 0.8))
p2 = spplot(formes, "iki", 
    col.regions = colorRampPalette(c('grey96', 'green4'))(30),  
    main = list(label = "Index pesticides par département", cex = 0.8))
grid.arrange(p1, p2, nrow = 1)
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, results = "asis", fig.cap = "Les valeurs moyennes par département, partie 2"}
# Plot
p1 = spplot(formes, "si", 
    col.regions = colorRampPalette(c('grey96', 'red'))(30),  
    main = list(label = "Surface qultivé par département", cex = 0.8))
p2 = spplot(formes, "ri", 
    col.regions = colorRampPalette(c('grey96', 'orange'))(30),  
    main = list(label = "Revenus par département", cex = 0.8))
grid.arrange(p1, p2, nrow = 1)
```

\FloatBarrier

\newpage

### A2 Les graphiques bivariés

#### Cas général 

\FloatBarrier

```{r echo = FALSE, fig.cap = "L'étude bivarié, partie 1"}
p1 = datai %>% 
    ggplot(aes(qi, ipi, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    # ggtitle("Q ~ IP") +
    xlab("Quantité du vin") + ylab("Index du prix") +
    pres_theme
p2 = datai %>% 
    ggplot(aes(qi, iki, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    # ggtitle("Q ~ IK") +
    xlab("Quantité du vin") + ylab("Index des pésticides") +
    pres_theme
grid.arrange(p1, p2, nrow = 1)
# Probably it is better to do it by variable
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, fig.cap = "L'étude bivarié, partie 2"}
p1 = datai %>% 
    ggplot(aes(qi, si, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    # ggtitle("Q ~ S") +
    xlab("Quantité du vin") + ylab("Surface qultivé") +
    pres_theme
p2 = datai %>% 
    ggplot(aes(qi, ri, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    # ggtitle("Q ~ R") +
    xlab("Quantité du vin") + ylab("Revenus") +
    pres_theme
grid.arrange(p1, p2, nrow = 1)
```

\FloatBarrier

#### Transformation *Within*

\FloatBarrier

```{r echo = FALSE, fig.cap = "Rélations bivariés dans le cas de transformation within, partie 1"}
p1 = dataWi %>% 
    ggplot(aes(qi, ipi, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    geom_smooth(aes(qi, ipi, col = "black"), method = "loess", size = 0.4) +
    # ggtitle("Q ~ IP") +
    xlab("Quantité du vin") + ylab("Index du prix") +
    pres_theme
p2 = dataWi %>% 
    ggplot(aes(qi, iki, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    geom_smooth(aes(qi, iki, col = "black"), method = "loess", size = 0.4) +
    # ggtitle("Q ~ IK") +
    xlab("Quantité du vin") + ylab("Index des pésticides") +
    pres_theme
grid.arrange(p1, p2, nrow = 1)
# Probably it is better to do it by variable
```

\FloatBarrier

```{r echo = FALSE, fig.cap = "Rélations bivariés dans le cas de transformation within, partie 2"}
p1 = dataWi %>% 
    ggplot(aes(qi, si, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    geom_smooth(aes(qi, si, col = "black"), method = "loess", size = 0.4) +
    # ggtitle("Q ~ S") +
    xlab("Quantité du vin") + ylab("Surface qultivé") +
    pres_theme
p2 = dataWi %>% 
    ggplot(aes(qi, ri, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    geom_smooth(aes(qi, ri, col = "black"), method = "loess", size = 0.4) +
    # ggtitle("Q ~ R") +
    xlab("Quantité du vin") + ylab("Revenus") +
    pres_theme
grid.arrange(p1, p2, nrow = 1)
```

\FloatBarrier

\newpage

### A3 La correlation 

#### Cas général 

Le premier tableau combrend les résultats pour les données telles-quelles, le deuxieme par contre integre les résultats pour les données sous la trasformation *within*.

```{r include = FALSE}
correlation = cor(datap)
colnames(correlation) = c("Quantité du vin", "IP", 
        "Surface", "Revenus", 
        "Index pésticides", "Temps")
rownames(correlation) = c("Quantité du vin", "IP", 
        "Surface", "Revenus", 
        "Index pésticides", "Temps")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
xtable(round(correlation, 4), 
    caption = "La correlation complete",
    align = "l|rrrrrr")
```

\FloatBarrier

#### Transformation *Within*

Les rélations entre les variables mieux ressortent pour les données transformées.

```{r include = FALSE}
correlationW = cor(dataW)
dataW$ndep = index(datap)$ndep
colnames(correlationW) = c("Quantité du vin", "IP", 
        "Surface", "Revenus", 
        "Index pésticides", "Temps")
rownames(correlationW) = c("Quantité du vin", "IP", 
        "Surface", "Revenus", 
        "Index pésticides", "Temps")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
xtable(round(correlationW, 4), 
    caption = "La correlation within",
    align = "l|rrrrrr")
```

\FloatBarrier

\newpage

## B Analyse des résultats OLS, WLS et SUR

```{r include = FALSE}
# Data transformation for systemfit
dataWX = as.data.frame(dataW)
```

```{r include = FALSE}
# equations
eqdemand = qi ~ 0 + ipi + ri
eqoffer = qi ~ 0 + ipi + si + iki 
system = list(Demande = eqdemand, Offre = eqoffer)
# OLS
ols = systemfit(system, 
    data = dataWX, 
    method = "OLS")
# WLS
wls = systemfit(system, 
    data = dataWX, 
    method = "WLS")
# SUR
sur = systemfit(system, 
    data = dataWX, 
    method = "SUR")
```

### B1 Independance des résidus  

```{r include = FALSE}
cordata = dataWX %>% 
    mutate(u1 = ols$eq[[1]]$res,
        u2 = ols$eq[[2]]$res,
        u3 = wls$eq[[1]]$res,
        u4 = wls$eq[[2]]$res,
        u5 = sur$eq[[1]]$res,
        u6 = sur$eq[[2]]$res)
cormat = cor(cordata[,-c(6,7)])[1:5, 6:11]
colnames(cormat) = c("OLS D", "OLS O", 
        "WLS D", "WLS O", 
        "SUR D", "SUR O")
rownames(cormat) = c("Vin", "IP", 
        "Surface", "Revenus", 
        "Pesticides")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
xtable(round(cormat, 4), 
    caption = "Correlation des résidus",
    align = "l|rrrrrr")
# cordata %>% ggplot(aes(x = u6, y = f6)) + geom_point() + geom_smooth()
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, fig.cap = "Les résidus contre la variable prédite"}
par(mfrow = c(1,2), cex = 0.5)
plot(y = ols$eq[[1]]$res, x = ols$eq[[1]]$fit,
    col = "blue", 
    main = "Demande", 
    ylab = "Residuals", xlab = "Fitted")
points(y = wls$eq[[1]]$res, x = wls$eq[[1]]$fit,
    col = "green4", pch = 17)
points(y = sur$eq[[1]]$res, x = sur$eq[[1]]$fit,
    col = "red", pch = 14)
plot(x = ols$eq[[2]]$res, y = ols$eq[[2]]$fit,
    col = "blue", 
    main = "Offre", 
    ylab = "Residuals", xlab = "Fitted")
points(y = wls$eq[[2]]$res, x = wls$eq[[2]]$fit,
    col = "green4", pch = 17)
points(y = sur$eq[[2]]$res, x = sur$eq[[2]]$fit,
    col = "red", pch = 14)
legend(x = "topright", legend = c("OLS", "WLS", "SUR"),
    col = c("blue", "green4", "red"), xpd = NA,
    cex = 1, lwd = 3)
```

\FloatBarrier

### B2 L'autocorrelation

```{r include = FALSE}
# Panel Durbin-Watson test
cordataDW = cordata %>% 
    group_by(ndep) %>%
    mutate(u1_diff = u1 - dplyr::lag(u1),
        u2_diff = u2 - dplyr::lag(u2),
        u3_diff = u3 - dplyr::lag(u3),
        u4_diff = u4 - dplyr::lag(u4),
        u5_diff = u5 - dplyr::lag(u5),
        u6_diff = u6 - dplyr::lag(u6)) %>%
    ungroup() %>%
    dplyr::select(contains("u")) %>%
    mutate_all(function(x) replace_na(x, 0)) %>%
    summarise_all(function(x) sum(x^2))
pDW = data.frame(ncol = 3, nrow = 2)
pDW[1,1] = cordataDW[1,1]/cordataDW[1,7]
pDW[2,1] = cordataDW[1,2]/cordataDW[1,8]
pDW[1,2] = cordataDW[1,3]/cordataDW[1,9]
pDW[2,2] = cordataDW[1,4]/cordataDW[1,10]
pDW[1,3] = cordataDW[1,5]/cordataDW[1,11]
pDW[2,3] = cordataDW[1,6]/cordataDW[1,12]
colnames(pDW) = c("OLS", "WLS", "SUR")
rownames(pDW) = c("Equation de demande", "Equation d'offre")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(pDW, 
    header = FALSE,
    title = "Les statistiques test de Durbin-Watson",
    summary = FALSE)
```

\FloatBarrier

### B3 Test de l'hétéroskedacité

```{r include = FALSE}
BartT = data.frame(ncol = 3, nrow = 2)
BartT[1,1] = ols_test_bartlett(cordata, u1, 
    group_var = ndep)$pval
BartT[2,1] = ols_test_bartlett(cordata, u2, 
    group_var = ndep)$pval
BartT[1,2] = ols_test_bartlett(cordata, u3, 
    group_var = ndep)$pval
BartT[2,2] = ols_test_bartlett(cordata, u4, 
    group_var = ndep)$pval
BartT[1,3] = ols_test_bartlett(cordata, u5, 
    group_var = ndep)$pval
BartT[2,3] = ols_test_bartlett(cordata, u6, 
    group_var = ndep)$pval
colnames(BartT) = c("OLS", "WLS", "SUR")
rownames(BartT) = c("Equation de demande", "Equation d'offre")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(BartT,
    header = FALSE,
    summary = FALSE,
    title = "Les résultat du test de Bartlett sur l'heteroscedacité")
```

\FloatBarrier

\newpage

### B4 La normalité des résidus 

\FloatBarrier

```{r include = FALSE}
ShapT = data.frame(nrow = 2, ncol = 6)
ShapT[1,1] = shapiro.test(ols$eq[[1]]$res)$p.val
ShapT[2,1] = shapiro.test(ols$eq[[2]]$res)$p.val
ShapT[1,2] = shapiro.test(wls$eq[[1]]$res)$p.val
ShapT[2,2] = shapiro.test(wls$eq[[2]]$res)$p.val
ShapT[1,3] = shapiro.test(sur$eq[[1]]$res)$p.val
ShapT[2,3] = shapiro.test(sur$eq[[2]]$res)$p.val
colnames(ShapT) = c("OLS", "WLS", "SUR")
rownames(ShapT) = c("Equation de demande", "Equation d'offre")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(ShapT,
    header = FALSE,
    summary = FALSE,
    title = "Shapiro-Wilk test de normalité des résidus")
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, fig.cap = "Les PDF des résidus"}
par(mfrow = c(1,2), cex = 0.5)
plot(density(ols$eq[[1]]$res), col = "blue", 
    main = "Demande", xlab = "Residuals")
lines(density(wls$eq[[1]]$res), col = "green4")
lines(density(sur$eq[[1]]$res), col = "red")
plot(density(sur$eq[[2]]$res), col = "red", 
    main = "Offre", xlab = "Residuals")
lines(density(wls$eq[[2]]$res), col = "green4")
lines(density(ols$eq[[2]]$res), col = "blue")
legend(x = "topright", legend = c("OLS", "WLS", "SUR"),
    col = c("blue", "green4", "red"), xpd = NA,
    cex = 1, lwd = 3)
```

\FloatBarrier

\newpage

## C Analyse des résultats 2SLS, W2SLS, 3SLS et i3SLS

\FloatBarrier

```{r include = FALSE}
# equations
eqdemand = qi ~ 0 + ipi + ri
eqoffer = qi ~ 0 + ipi + si + iki 
inst = ~ ri + si + iki
system = list(Demande = eqdemand, Offre = eqoffer)
# 2SLS
# 2SLS is an equivalent of ILS (indirect least squares)
sls2 = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "2SLS")
# 2WSLS
wsls2 = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "W2SLS")
# 3SLS (errors correction)
sls3 = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "3SLS")
# FIML (iterated 3SLS)
fiml = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "3SLS", maxit = 1000)
```

\FloatBarrier

### C1 Independance des résidus  

```{r include = FALSE}
resdata3 = dataWX %>% 
    mutate(u1 = sls2$eq[[1]]$res,
        u2 = sls2$eq[[2]]$res,
        u3 = sls3$eq[[1]]$res,
        u4 = sls3$eq[[2]]$res,
        u5 = fiml$eq[[1]]$res,
        u6 = fiml$eq[[2]]$res)
```

```{r include = FALSE}
cormat = cor(resdata3[,-c(6,7)])
cormat = cormat[1:5, 6:11]
colnames(cormat) = c("2SLS D", "2SLS O", 
        "3SLS D", "3SLS O",
        "i3SLS D", "i3SLS O")
rownames(cormat) = c("Vin", "IP", 
        "Surface", "Revenus", 
        "Pesticides")
```

\FloatBarrier

```{r include = FALSE}
xtable(round(cormat, 4), 
    caption = "Correlation des résidus",
    align = "l|rrrrrr")
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, fig.cap = "Les résidus contre la variable prédite"}
par(mfrow = c(1,2), cex = 0.5)
plot(y = sls2$eq[[1]]$res, x = sls2$eq[[1]]$fit,
    col = "blue", 
    main = "Demande", 
    ylab = "Residuals", xlab = "Fitted")
points(y = fiml$eq[[1]]$res, x = fiml$eq[[1]]$fit,
    col = "green4", pch = 17)
points(y = sls3$eq[[1]]$res, x = sls3$eq[[1]]$fit,
    col = "red", pch = 14)
plot(y = sls2$eq[[2]]$res, x = sls2$eq[[2]]$fit,
    col = "blue", 
    main = "Offre", 
    ylab = "Residuals", xlab = "Fitted")
points(y = fiml$eq[[2]]$res, x = fiml$eq[[2]]$fit,
    col = "green4", pch = 17)
points(y = sls3$eq[[2]]$res, x = sls3$eq[[2]]$fit,
    col = "red", pch = 14)
par(xpd = NA)
legend(x = "topright", legend = c("2SLS", "i3SLS", "3SLS"),
    col = c("blue", "green4", "red"), xpd = NA,
    cex = 1, lwd = 3)
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, fig.cap = "Les résidus et les prédictions, le cas de i3SLS"}
par(mfrow = c(1,2), cex = 0.5)
plot(y = fiml$eq[[1]]$res, x = as.numeric(resdata3$qi),
    col = "red", pch = 17, 
    main = "Demande", 
    ylab = "Fitted - résidus", xlab = "Real")
points(y = fiml$eq[[1]]$fit, x = resdata3$qi, 
    col = "blue", pch = 17)
lines(y = fiml$eq[[1]]$fit + fiml$eq[[1]]$res, x = resdata3$qi,
    col = "black")
legend(x = "bottomleft", legend = c("Résidus", "Fitted", "Real"),
    col = c("red", "blue", "black"), xpd = NA,
    cex = 1, lwd = 3)
plot(y = fiml$eq[[2]]$fit, x = as.numeric(resdata3$qi), 
    col = "blue", pch = 17, 
    main = "Offre", 
    ylab = "Fitted - résidus", xlab = "Real")
points(y = fiml$eq[[2]]$res, x = resdata3$qi,
    col = "red", pch = 14)
lines(y = fiml$eq[[2]]$fit + fiml$eq[[2]]$res, x = resdata3$qi,
    col = "black")
```

\FloatBarrier

\newpage

### C2 L'autocorrelation 

```{r include = FALSE}
# Panel Durbin-Watson test
# Create dataframe
resdataDW = resdata3 %>% 
    group_by(ndep) %>%
    mutate(u1_diff = u1 - dplyr::lag(u1),
        u2_diff = u2 - dplyr::lag(u2),
        u3_diff = u3 - dplyr::lag(u3),
        u4_diff = u4 - dplyr::lag(u4),
        u5_diff = u5 - dplyr::lag(u5),
        u6_diff = u6 - dplyr::lag(u6)) %>%
    ungroup() %>%
    dplyr::select(contains("u")) %>%
    mutate_all(function(x) replace_na(x, 0)) %>%
    summarise_all(function(x) sum(x^2))
# Calculate test statistics
pDW = data.frame(ncol = 3, nrow = 2)
pDW[1,1] = resdataDW[1,1]/resdataDW[1,7]
pDW[2,1] = resdataDW[1,2]/resdataDW[1,8]
pDW[1,2] = resdataDW[1,3]/resdataDW[1,9]
pDW[2,2] = resdataDW[1,4]/resdataDW[1,10]
pDW[1,3] = resdataDW[1,5]/resdataDW[1,11]
pDW[2,3] = resdataDW[1,6]/resdataDW[1,12]
# Rename
colnames(pDW) = c("2SLS", "3SLS", "i3SLS")
rownames(pDW) = c("Equation de demande", "Equation d'offre")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(pDW,
    header = FALSE,
    title = "Les resultats du test de Durbin-Watson",
    summary = FALSE)
```

\FloatBarrier

### C3 Test de l'hétéroskedacité

```{r include = FALSE}
resdata3 = dataWX %>% 
    mutate(u1 = sls2$eq[[1]]$res,
        u2 = sls2$eq[[2]]$res,
        u3 = sls3$eq[[1]]$res,
        u4 = sls3$eq[[2]]$res,
        u5 = fiml$eq[[1]]$res,
        u6 = fiml$eq[[2]]$res)
```

```{r include = FALSE}
BartT = data.frame(ncol = 3, nrow = 2)
BartT[1,1] = ols_test_bartlett(resdata3, u1, group_var = ndep)$pval
BartT[2,1] = ols_test_bartlett(resdata3, u2, group_var = ndep)$pval
BartT[1,2] = ols_test_bartlett(resdata3, u3, group_var = ndep)$pval
BartT[2,2] = ols_test_bartlett(resdata3, u4, group_var = ndep)$pval
BartT[1,3] = ols_test_bartlett(resdata3, u5, group_var = ndep)$pval
BartT[2,3] = ols_test_bartlett(resdata3, u6, group_var = ndep)$pval
colnames(BartT) = c("2SLS", "3SLS", "i3SLS")
rownames(BartT) = c("Equation de demande", "Equation d'offre")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(BartT,
    summary = FALSE,
    title = "Test de Bartlett sur l'heterockedacité")
```

\FloatBarrier

\newpage

### C4 La normalité des résidus 

\FloatBarrier

```{r include = FALSE}
ShapT = data.frame(nrow = 2, ncol = 6)
ShapT[1,1] = shapiro.test(sls2$eq[[1]]$res)$p.val
ShapT[2,1] = shapiro.test(sls2$eq[[2]]$res)$p.val
ShapT[1,2] = shapiro.test(sls3$eq[[1]]$res)$p.val
ShapT[2,2] = shapiro.test(sls3$eq[[2]]$res)$p.val
ShapT[1,3] = shapiro.test(fiml$eq[[1]]$res)$p.val
ShapT[2,3] = shapiro.test(fiml$eq[[2]]$res)$p.val
colnames(ShapT) = c("2SLS", "3SLS", "i3SLS")
rownames(ShapT) = c("Equation de demande", "Equation d'offre")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(ShapT,
    header = FALSE,
    summary = FALSE,
    title = "Shapiro-Wilk test de normalité")
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, fig.cap = "Les PDF des résidus"}
par(mfrow = c(1,2), cex = 0.5)
plot(density(sls2$eq[[1]]$res), col = "blue", 
    main = "Demande", xlab = "Residuals")
lines(density(fiml$eq[[1]]$res), col = "green4")
lines(density(sls3$eq[[1]]$res), col = "red")
plot(density(sls3$eq[[2]]$res), col = "red", 
    main = "Offre", xlab = "Residuals")
lines(density(fiml$eq[[2]]$res), col = "green4")
lines(density(sls2$eq[[2]]$res), col = "blue")
legend(x = "topright", legend = c("2SLS", "i3SLS", "3SLS"),
    col = c("blue", "green4", "red"), xpd = NA,
    cex = 1, lwd = 3)
```

\FloatBarrier

### C5 Comparaison des modèles  

\FloatBarrier

```{r include = FALSE}
h1 = hausman.systemfit(sls2, sls3) # p = 1, 3SLS inconsistent
# 2SLS estimator is consistent
h2 = hausman.systemfit(sls2, fiml)
res = data.frame(Test = c("2SLS contre 3SLS", "2SLS contre i3SLS"),
    Resultats = c(h1$p.val, h2$p.val))
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(res, 
    title = "Hausman 3SLS consistency test",
    summary = F, 
    header = F)
``` 

\FloatBarrier

```{r include = FALSE}
# - Linear test :
# What should be tested ???
# linearHypothesis(sls2, 
```
\FloatBarrier

```{r include = FALSE}
lr = lrtest(sls2, sls3, fiml)
xtable(round(lr, 4), 
    caption = "ML test de spécification",
    align = "c|ccccc")
```

\FloatBarrier

\newpage 

## D Clusterisation 

### D1 *Between* transformation 

Les groupes sont définies par des caractéristiques suivantes :

\FloatBarrier

```{r echo = FALSE, results = "asis"}
clusters1 = cbind(fit1$centers, fit1$size)
names(clusters1) = c("Groupe", "Quantité", "IP", 
    "Surface", "Revenus", "Index pesticides", "n ")
stargazer(clusters1, 
    title = "Les centres des clusters",
    summary = F, 
    header = F)
```

\FloatBarrier

### D2 *Within* transformation 

#### Les centres

Les groupes sont définies par des caractéristiques suivantes :

```{r include = FALSE}
names(clusters2)[(ncol(clusters2)-1):ncol(clusters2)] = c("n", "k")
c1 = clusters2 %>%
    dplyr::select(qi, ipi, si, ri, iki, n, k) %>%
    mutate(t = 1)
c2 = clusters2 %>%
    dplyr::select(ends_with("t2"), n, k) %>%
    mutate(t = 2)
names(c2)[1:5] = c("qi", "ipi", "si", "ri", "iki")
c3 = clusters2 %>%
    dplyr::select(ends_with("t3"), n, k) %>%
    mutate(t = 3)
names(c3)[1:5] = c("qi", "ipi", "si", "ri", "iki")
c4 = clusters2 %>%
    dplyr::select(ends_with("t4"), n, k) %>%
    mutate(t = 4)
names(c4)[1:5] = c("qi", "ipi", "si", "ri", "iki")
c5 = clusters2 %>%
    dplyr::select(ends_with("t5"), n, k) %>%
    mutate(t = 5)
names(c5)[1:5] = c("qi", "ipi", "si", "ri", "iki")
centers2 = rbind(c1, c2, c3, c4, c5) %>% 
    group_by(k) %>% 
    arrange(t, .by_group = T)
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
# Print results
stargazer(round(centers2, 6), 
    title = "Les centres des clusters",
    summary = F, 
    header = F)
```

\FloatBarrier

#### Representation graphique

```{r echo = FALSE, results = "asis"}
# Plot 
p1 = centers2 %>% ggplot(aes(y = qi, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("Quantité") +
    ggtitle("Q~T") +
    pres_theme
p2 = centers2 %>% ggplot(aes(y = ipi, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("IP") +
    ggtitle("IP~T") +
    pres_theme
p3 = centers2 %>% ggplot(aes(y = iki, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("Index pesticides") +
    ggtitle("IK~T") +
    pres_theme
p4 = centers2 %>% ggplot(aes(y = ri, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("Revenus") +
    ggtitle("R~T") +
    pres_theme
# Arrange
grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)
```

### D3 Cas d'information complete 

#### Les centres

Les groupes sont définies par des caractéristiques suivantes :

```{r include = FALSE}
names(clusters3)[(ncol(clusters3)-1):ncol(clusters3)] = c("n", "k")
c1 = clusters3 %>%
    dplyr::select(qi, ipi, si, ri, iki, n, k) %>%
    mutate(t = 1)
c2 = clusters3 %>%
    dplyr::select(ends_with("t2"), n, k) %>%
    mutate(t = 2)
names(c2)[1:5] = c("qi", "ipi", "si", "ri", "iki")
c3 = clusters3 %>%
    dplyr::select(ends_with("t3"), n, k) %>%
    mutate(t = 3)
names(c3)[1:5] = c("qi", "ipi", "si", "ri", "iki")
c4 = clusters3 %>%
    dplyr::select(ends_with("t4"), n, k) %>%
    mutate(t = 4)
names(c4)[1:5] = c("qi", "ipi", "si", "ri", "iki")
c5 = clusters3 %>%
    dplyr::select(ends_with("t5"), n, k) %>%
    mutate(t = 5)
names(c5)[1:5] = c("qi", "ipi", "si", "ri", "iki")
centers3 = rbind(c1, c2, c3, c4, c5) %>% 
    group_by(k) %>% 
    arrange(t, .by_group = T)
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
# Print 
stargazer(round(centers3, 6), 
    title = "Les centres des clusters",
    summary = F, 
    header = F)
```

\FloatBarrier

#### Representation graphique

\FloatBarrier

```{r echo = FALSE, results = "asis"}
# Plot 
p1 = centers3 %>% ggplot(aes(y = qi, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("Quantité") +
    ggtitle("Q~T") +
    pres_theme
p2 = centers3 %>% ggplot(aes(y = ipi, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("IP") +
    ggtitle("IP~T") +
    pres_theme
p3 = centers3 %>% ggplot(aes(y = iki, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("Index pesticides") +
    ggtitle("IK~T") +
    pres_theme
p4 = centers3 %>% ggplot(aes(y = ri, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("Revenus") +
    ggtitle("R~T") +
    pres_theme
# Arrange
grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)
```

\FloatBarrier

## E Analyse des résultats pour information clusterisée OLS, 2SLS et 3SLS

Etudions la validité du modèle 3SLS : 

```{r include = FALSE}
resdata3x = dataWZ %>% 
    mutate(u1 = olsx$eq[[1]]$res,
        u2 = olsx$eq[[2]]$res,
        u3 = sls2x$eq[[1]]$res,
        u4 = sls2x$eq[[2]]$res,
        u5 = sls3x$eq[[1]]$res,
        u6 = sls3x$eq[[2]]$res)
```

\FloatBarrier

```{r include = FALSE}
h1 = hausman.systemfit(sls2x, sls3x) # p = 1, 3SLS inconsistent
# 2SLS estimator is consistent
res = data.frame(Test = c("2SLS contre 3SLS"),
    Resultats = c(h1$p.val)) 
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(res, 
    title = "Hausman 3SLS consistency test",
    summary = F, 
    header = F)
```

La normalité des résidus :

```{r include = FALSE}
ShapT = data.frame(nrow = 2, ncol = 6)
ShapT[1,1] = shapiro.test(olsx$eq[[1]]$res)$p.val
ShapT[2,1] = shapiro.test(olsx$eq[[2]]$res)$p.val
ShapT[1,2] = shapiro.test(sls2x$eq[[1]]$res)$p.val
ShapT[2,2] = shapiro.test(sls2x$eq[[2]]$res)$p.val
ShapT[1,3] = shapiro.test(sls3x$eq[[1]]$res)$p.val
ShapT[2,3] = shapiro.test(sls3x$eq[[2]]$res)$p.val
colnames(ShapT) = c("OLS", "2SLS", "3SLS")
rownames(ShapT) = c("Equation de demande", "Equation d'offre")
```
\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(ShapT,
    summary = FALSE,
    title = "Shapiro-Wilk normality test",
    header = F)
```

\FloatBarrier

L'heteroscedacité :

\FloatBarrier

```{r include = FALSE}
BartT = data.frame(ncol = 3, nrow = 2)
BartT[1,1] = ols_test_bartlett(resdata3x, u1, 
    group_var = ndep)$pval
BartT[2,1] = ols_test_bartlett(resdata3x, u2, 
    group_var = ndep)$pval
BartT[1,2] = ols_test_bartlett(resdata3x, u3, 
    group_var = ndep)$pval
BartT[2,2] = ols_test_bartlett(resdata3x, u4, 
    group_var = ndep)$pval
BartT[1,3] = ols_test_bartlett(resdata3x, u5, 
    group_var = ndep)$pval
BartT[2,3] = ols_test_bartlett(resdata3x, u6, 
    group_var = ndep)$pval
colnames(BartT) = c("OLS", "2SLS", "3SLS")
rownames(BartT) = c("Equation de demande", "Equation d'offre")
```
\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(BartT,
    summary = FALSE,
    title = "Bartlett heteroscedasticity test",
    header = F)
```

\FloatBarrier

Les PDF des résidus :

\FloatBarrier

```{r echo = FALSE}
par(mfrow = c(1,2), cex = 0.5)
plot(density(olsx$eq[[1]]$res), col = "blue", 
    main = "Demande", xlab = "Residuals")
lines(density(sls2x$eq[[1]]$res), col = "green4")
lines(density(sls3x$eq[[1]]$res), col = "red")
plot(density(sls3x$eq[[2]]$res), col = "red", 
    main = "Offre", xlab = "Residuals")
lines(density(sls2x$eq[[2]]$res), col = "green4")
lines(density(olsx$eq[[2]]$res), col = "blue")
legend(x = "topright", legend = c("OLS", "2SLS", "3SLS"),
    col = c("blue", "green4", "red"), xpd = NA,
    cex = 1, lwd = 3)
```

\FloatBarrier

Les résidus contre les variables prédites : 

\FloatBarrier

```{r echo = FALSE}
par(mfrow = c(1,2), cex = 0.5)
plot(y = sls3x$eq[[1]]$res, x = sls3x$eq[[1]]$fit,
    col = "red", 
    main = "Demande", 
    ylab = "Residuals", xlab = "Fitted")
points(y = sls2x$eq[[1]]$res, x = sls2x$eq[[1]]$fit,
    col = "green4", pch = 17)
points(y = olsx$eq[[1]]$res, x = olsx$eq[[1]]$fit,
    col = "blue", pch = 14)
plot(y = sls3x$eq[[2]]$res, x = sls3x$eq[[2]]$fit,
    col = "red", 
    main = "Offre", 
    ylab = "Residuals", xlab = "Fitted")
points(y = sls2x$eq[[2]]$res, x = sls2x$eq[[2]]$fit,
    col = "green4", pch = 17)
points(y = olsx$eq[[2]]$res, x = olsx$eq[[2]]$fit,
    col = "blue", pch = 14)
par(xpd = NA)
legend(x = "topright", legend = c("OLS", "2SLS", "3SLS"),
    col = c("blue", "green4", "red"), xpd = NA,
    cex = 1, lwd = 3)
```

\FloatBarrier

\newpage

## F Dictionnaire des variables 

Finalement, nous offrons au lecteur un tableau de reference pour notre base des données finale.

\FloatBarrier

\begin{table}[!htbp]
  \centering
\caption{Ditionnaire des varibales}
\begin{tabular}{c|l}
  \noindent\rule[0.5ex]{\linewidth}{1pt}
  Variable & Description \\
  \noindent\rule[0.5ex]{\linewidth}{1pt}
année & année \\
ndep & numéro de département \\
si & superficie de vigne sans indication géographique en hectare en log \\
qi & quantité de vins produits en hectolitre en log \\
ipi & indice des prix du vin sans IG déflatés en log  \\
ri & revenu disponible brut des ménages français déflatés en log \\
iki & indice de quantité de pesticides achetés en log \\
t & la tendance temporelle \\
\noindent\rule[0.5ex]{\linewidth}{1pt}
\end{tabular}
\end{table}

\FloatBarrier

\newpage

# References 

```{r eval = FALSE, include = FALSE}
#####################################################
###################   References   ##################
#####################################################
```
