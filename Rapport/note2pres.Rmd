---
title: "Etude des effets des pésticides dans la production des vins de table"
shorttitle : "Les effets des pésticides"
subtitle: "Analyse empirique des marchés"
author: Arnaud Blanc, Nikita Gusarov, Sasha Picon
shortauthor : A.Blanc, N.Gusarov, S.Picon
institute: Université Grenoble Alpes
shortinstitute: UGA
date: 25/12/2019  
# header-includes:
#     - \usepackage{array}
#     - \usepackage{multicol}
#     - \usepackage{graphicx}
#     - \usepackage{placeins}
#     - \usepackage{xcolor}
output: 
    pdf_document:
        # template: Template.tex
        # slide_level: 2
        # fonttheme: "structurebold"
        toc: false
        # toc_depth: 1
        df_print: "kable"
        fig_width: 6
        fig_height: 3
        fig_caption: yes
        number_sections: FALSE
        includes:
            in_header: packages.sty
            before_body: toc.sty
fontsize: 11pt
bibliography: biblio.bib
# geometry: margin = 0.5in
---

```{r include = FALSE}
###################
# Setting r options
###################
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(size = "tiny")
knitr::opts_chunk$set(dpi = 600)
knitr::opts_chunk$set(fig.align = "center") 
knitr::opts_chunk$set(fig.pos = "!htbp")
```

```{r include = FALSE, eval = FALSE}
#########################
# Extract R code from Rmd
#########################
# require(knitr)
# purl("./Presentation/Presentation.Rmd", 
#     output = "./Presentation/CodePresentation.R", 
#     documentation = 0)
```

```{r include = FALSE}
##########
# Packages
##########
# Results printing
require(stargazer)
require(texreg)
require(xtable)
# Statistics
require(systemfit) # System equations
require(olsrr) # Stat tests
require(plm) # Panel data
require(Formula)
# Plots 
require(gridExtra)
# Maps
require(raster)
# General packages
require(tidyverse)
require(rlang)
require(dummies)
require(dplyr)
```

```{r include = FALSE}
##############
# Loading data
##############
# Data
data = read.csv("../Donnees/Base-de-donnees-indice-prix.csv")
# names(data)
# Arrange
dn = data %>%
    filter(s_vin_simple != 0 & 
        (q_rouge + q_blanc) != 0 &
        (qk_prod + ql_prod) != 0 &
        IP != 0) %>%
    na.omit() %>%
    group_by(ndep) %>%
    count() %>% 
    filter(n == 5) # %>%
    # dplyr::select(ndep)
datax = data %>% 
    filter(ndep %in% dn$ndep) 
datay = datax %>% 
    filter(annee == 2012) %>%
    mutate(refqki = qk_prod + ql_prod) %>% 
    dplyr::select(ndep, refqki) 
datax = left_join(datax, datay)
datax = datax %>%
    mutate(IQK = (qk_prod + ql_prod)/refqki)
```

```{r include = FALSE}
##################
# Transformed data
##################
datai = datax %>%
    arrange(ndep) %>%
    mutate(si = log(s_vin_simple + 0.001), 
        qi = log(q_blanc + q_rouge + 0.001), 
        ipi = IP,
        ri = log(revenu.déflaté),
        iki = IQK,
        t = as.integer(as.factor(annee)),
        year = annee) %>%
    dplyr::select(year, ndep, qi, ipi, si, ri, iki, t)
```

```{r include = FALSE}
############
# Panel data
############
datap = pdata.frame(datai, index = c("ndep", "year"),
    drop.index = T)
```

```{r include = FALSE}
###################
# Support functions
###################
# STATA version
#######
# xtsum (overall, within and between variance) for panel data
#######
xtsum = function(data, varname, unit) {
    # the variable to xtsum over
    varname = enquo(varname)
    # the identifier dimention
    loc.unit = enquo(unit)
    # overall
    ores = data %>% 
        summarise(ovr.mean = mean(!! varname, na.rm = TRUE), 
        ovr.sd = sd(!! varname, na.rm = TRUE), 
        ovr.min = min(!! varname, na.rm = TRUE), 
        ovr.max = max(!! varname, na.rm = TRUE), 
        ovr.N = sum(as.numeric((!is.na(!! varname)))))
    # between
    bmeans = data %>% 
        group_by(!! loc.unit) %>% 
        summarise(meanx = mean(!! varname, na.rm = TRUE), 
        t.count = sum(as.numeric(!is.na(!! varname))))
    bres = bmeans %>% 
        ungroup() %>% 
        summarise(between.sd = sd(meanx, na.rm = TRUE), 
        between.min = min(meanx, na.rm = TRUE), 
        between.max = max(meanx, na.rm = TRUE), 
        units = sum(as.numeric(!is.na(t.count))), 
        t.bar = mean(t.count, na.rm = TRUE))
    # within
    wdat = data %>% 
        group_by(!! loc.unit) %>% 
        mutate(W.x = scale(!! varname, scale = FALSE))
    wres = wdat %>% 
        ungroup() %>%  
        summarise(within.sd = sd(W.x, na.rm = TRUE), 
        within.min = min(W.x, na.rm = TRUE), 
        within.max = max(W.x, na.rm = TRUE))
    # results
    return(list(var = varname, ores = ores, bres = bres, wres = wres))
}
####################################
# Print results for a list of xtsums
####################################
print.xtsum = function(xtsums.list) {
    # takes multiple xtsums as list
    df = data.frame(Variable = NA, Mean = NA,
        Overall = NA, Between = NA, Within = NA)
    # Filling loop
    for (i in 1:length(xtsums.list)) {
        df[i,1] = as_name(xtsums.list[[i]]$var)
        df[i,2] = xtsums.list[[i]]$ores$ovr.mean 
        df[i,3] = xtsums.list[[i]]$ores$ovr.sd
        df[i,4] = xtsums.list[[i]]$bres$between.sd
        df[i,5] = xtsums.list[[i]]$wres$within.sd
    }
    # Rownames
    rownames(df) = df[,1]
    # Results
    return(df = df[,-1])
}
#################
# Effects testing 
#################
Effect.testing = function(Formulas, data) {
    Dtest = data.frame(var = 0,
        Random = 0, Fixed = 0, 
        Individual = 0, Time = 0, Twoways = 0)
        for (i in 1:length(Formulas)) {
            Dtest[i,1] = names(Formulas)[i]
            ## Chow test
            # Random coefs for random effects          
            Dtest[i,2] = pooltest(Formulas[[i]],
                data = data,
                model = "random")$p.val 
            # Different coefs for fixed effects
            Dtest[i,3] = pooltest(Formulas[[i]],
                data = data,
                model = "within")$p.val
            ## Lagrange multiplier tests
            # Individual effects
            Dtest[i,4] = plmtest(Formulas[[i]],
                data = data,
                effect = "individual",
                type = "bp")$p.val
            # Time effects
            Dtest[i,5] = plmtest(Formulas[[i]],
                data = data,
                effect = "time",
                type = "bp")$p.val
            # Two-ways effects (individual and time)
            Dtest[i,6] = plmtest(Formulas[[i]],
                data = data,
                effect = "twoways",
                type = "ghm")$p.val   
        }
    rownames(Dtest) = Dtest[,1]
    return(Dtest = Dtest[,-1])
}
```

```{r include = FALSE}
##################
# Set ggplot style
##################
pres_theme = theme(text = element_text(size = rel(3)),
    legend.position = "none")
```

# Introduction

```{r eval = FALSE, include = FALSE}
####################################################
################### Introduction ###################
####################################################
```

Aujourd’hui, l’utilisation des pesticides est un problème majeur de l’agriculture.  
Celle-ci utilise la majeure partie des pesticides en France. 
Il s’agit d’un enjeu majeur du développement durable car ils ont un impact important sur les risques environnementaux et sanitaires. 

Les pesticides sont utilisés dans l’agriculture pour protéger la production. 
Il est supposer que les pesticides servent à protéger les rendements. 
En effet, les aléas climatiques influencent sur le développement de champignons ou de maladie. 
Ainsi, les pesticides permettent de protéger les cultures contre les aléas climatiques et de ne pas perdre de production. 

Dans ce travail nous cherchons à comprendre et estimer les effets des pésticides sur le marché des vins simples.
De cette façon nous chercherons à étudier l'équilibre sur le marché des vins simples ce qui est sensé de nous donner des résultats plus précis et fiables.

# 1. Les pesticides

```{r eval = FALSE, include = FALSE}
####################################################
###################  Pesticides  ###################
####################################################
```

\noindent\rule[0.5ex]{\linewidth}{1pt} 

\textcolor{red}{Mettre des sources partout !}

\noindent\rule[0.5ex]{\linewidth}{1pt}

Pour lutter contre l’utilisation des pesticides l’Etat Français et l’union européenne ont mis en place des mesures. 
Ainsi, l’Etat Français lors du grenelle de l’environnement de 2006 a fixé ces objectifs. 
Ainsi, le plan ECOPHYTO 2018 visait à réduire de 50% l’utilisation des pesticides de synthèse. 
Le deuxième objectif est le passage en agriculture biologique à 6% de la surface agricole utilisée en 2010 et vise 20% en 2020. 

En 2008, les 30 produits les plus toxiques les plus toxiques sont interdits. 
Une taxe sur les phytosanitaires a aussi été mise en place. 
Cette taxe est croissante avec leur niveau de toxicité. 
Cette taxe devait augmenter au fil des années et l’octroi de crédits d’impôt en faveur de l’agriculture biologique.

Malgré tous ces efforts, l’utilisation des pesticides perdurent. 
La France enregistre une hausse des ventes de produits phytosanitaires de 10000 tonnes, entre 2011 et 2016. 
En 2008, le nombre de doses unités a été créé pour enregistrer l’évolution de la demande de pesticide. 
On remarque que les doses utilisées augmentent de 12% en 2014-2016 par rapport à 2009-2011.  

## Etat actuel

Contrairement aux attentes des autorités, on ne remarque aucune baisse de l’utilisation de pesticides. 
Le Nodu a connu une hausse de 23% entre 2008 et 2017. 
Certaines critiques ont été faites sur l’utilisation du Nodu. 
Il est possible d’utiliser le nombre de substances actives utilisées. 
Mais, cet indicateur connaît lui aussi une hausse de 15% entre 2011 et 2017. 

Néanmoins, les politiques ont quand même eu quelques effets positifs, puisque l’achat des produits les plus dangereux baisse de 6% en 2017.  
Les grandes cultures sont les premières utilisatrices de pesticides. 
Elles représentent 67,4% de l’utilisation de pesticides. 
La deuxième culture est celle de la vigne ce qui représente 14,4%  des pesticides utilisés.

## Comment baisser l'utilisation de pesticides

Afin de baisser l’utilisation des pesticides, des méthodes de cultures ont été développées pour baisser l’utilisation des pesticides. 
Il est possible d’utiliser différents mode de culture. 
On peut en retenir trois principaux. 

Le premier est l’agriculture intensive. 
Elle ne limite pas le recours aux pesticides. 

Le deuxième est l’agriculture raisonnée. 
Elle limite le recours aux pesticides en fonction de seuils.
 
Le troisième niveau est l’agriculture biologique. 
Elle supprime les traitements avec des produits phytosanitaires de synthèse. 

Les professionnels proposent de commencer par utiliser l’agriculture raisonnée qui permet de réduire les doses de pesticides légales. 
Ensuite l’agriculture doit se déplacer vers l’agriculture biologique qui n’utilise aucun produit phytosanitaire de synthèse. 

# 2. Le marché du vin français

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Viticulture  ###################
#####################################################
```

La France est l’un des principaux producteurs et vendeurs de vin dans le monde. En effet, la France représente 10% de la surface de vigne dans le monde. 
La surface de vigne française se répartit dans 65 des 95 départements de la métropole. 
En France, il y a plus de 750000 hectares de vignes qui sont exploitées en 2018. 

Ainsi, en France, une exploitation agricole sur cinq est une exploitation viticole. Cela représente 85000 exploitations. 
La production de vins en France, représentait 4,6 milliard de litres. 
Cela représentait plus de 17% de la production totale de vin. 
En volume de production la France se place donc en deuxième position derrière le volume de production de l’Italie. 
3% de la surface agricole est consacrée à la production de vin. 
Néanmoins, le vin représente 15% de la production agricole en valeur. 

Du côté du consommateur, la France est le deuxième pays consommateur de vin derrière les Etats Unis. 
En effet, la consommation de vin en France représentait plus de 3,5 milliards de bouteille, en 2018. 
Néanmoins, on remarque une baisse de la consommation Française depuis une trentaine d’année.

## Le problème d'heterogénéité

Il existe une forte hétérogénéité entre les différents labels mais aussi à l'intérieur de ces labels. 

Dans le commerce du vin, il est courant de diviser les vins en deux grandes classes en fonction de leurs prix [@cembalo2014] : 

- les vins de qualité inférieure, les moins chers avec les caractéristiques de qualité de base ;
- les vins de qualité supérieure plus chers, dotés de caractéristiques qualitatives complexes et d'une image de grande valeur.

De plus, pour les vins français, selon @steiner2004, le système européen de classification des "*vins de qualité produits dans certaines régions*" (VQPRD) contient à la fois des vins AOC et des "*vins de haute qualité provenant d'un vignoble régional agréé*" (VDQS). 
Les vins de cépage appartiennent à la catégorie des vins autres que VQPRD, qui comprend les \textbf{vins de table} et les \textbf{vins de pays}.

En tenant compte des spécificités du marhcé du vin français, nous utilisons la méthodologie du ministère d'agriculture et divisons le marché en deux parties :

- La gamme haute (les vins IGP, vendus dans des magasins spécifiques) ;
- La gamme basse (les vins non IGP, vendus en grands surfaces).

La première partie est soumise à des règlements spécifiques : limitations des quantités produites, origine contrôlé, un caractère de la demande spécifique. 
La deuxième, c'est-à-dire le marché des vins moins chers, est aussi complexe. Les produits classés dans cette catégorie sont susceptibles d'avoir un certain degré d'hétérogénéité, comme cela a été montré par @cembalo2014.

## Les vins de table

Ces vins sans indication géographique (sans IG) ont vu leurs transactions augmenter en volume pour toutes les couleurs. 
Ainsi, on remarque que pour les vins rouges les transactions ont augmenté de 10%, pour les rosées la hausse représentait 52%, pour les vins blancs les volumes de transactions ont presque été doublé. 
Néanmoins on remarque egalement une baisse des cours des vins sans indication géographique. 

En effet, on remarque que les prix moyens pour les vins rouges et rosées sans indication géographique baisse de 3%. 
Le prix moyen des vins blancs baisse quand à eux de 12%, pour la campagne 2019/2020. 
Sur les deux mois de campagne, les échanges de Vin sans indication géographique est de 142 milliers d’hectolitres. 
Cela correspond à une hausse de 39% par rapport à la campagne précédente. 
Les ventes représentent 92 milliers d’hectolitres. 

La tendance sur le marché des vins sans indication géographique s’explique par une forte hausse des vins blancs. 
En effet, ceux-ci connaissent une hausse de près de 28 milliers d’hectolitres, soit une hausse de 232% vis-à-vis de la campagne de 2018-2019. Les vins rosés connaissent également une hausse. 
Néanmoins, celle-ci reste modeste puisque les ventes augmentaient de 61% par rapport à la campagne 2018/2019. 
En même temps, les ventes de vins rouges ont légèrement baissé. 
Le cours des Vins sans indication géographique baisse par rapport à la campagne précédente. 

Lors de la campagne 2018/2019, les ventes de vins en grande distribution sont en baisse. 
Cela peut s’expliquer par une hausse des prix moyens. 
Les ventes de vins représentent 8,7 millions d’hectolitres et un chiffre d’affaires de 4,1 milliards d’euros avec un prix moyen de 4,73 euro/litre. 
La baisse de la consommation de vins rouges s’aggrave avec une baisse de 8% par rapport à la campagne de 2017/2018. 
Les vins blancs connaissent aussi une faible baisse de 1,2% en volume par rapport à la consommation de la campagne précédente. 
Pour finir, les ventes de vins rosés ont baissé lors de la campagne 2018/2019. 
En effet, on enregistre une baisse de 3,9% en volume par rapport à la campagne 2017/2018. 
La consommation de vin sans indication géographique est de 6% en volume contre 3% en valeur. 
Les ventes de vins sans indications géographiques sont en légère hausse dans la campagne 2018/2019 par rapport à la campagne 2017/2018. 

Dans notre étude, nous traitons uniquement les vins simples (non IGP). 
La situation sur ce marché est sensée influencer l'utilisation des pesticides, car les volumes de productions sont plus significatives que pour le marché des vins IGP.
 
Suivant le raisonnement des chercheurs [@cembalo2014], dans une catégorie de vin avec une fourchette de prix étroite, il existe une homogénéité presque parfaite due à des vins ayant des attributs intrinsèques simples, une complexité de qualité médiocre et donc une différenciation peu marquée.
 
Cela nous permet d'analyser le marché par département est non par des marques/produits.

\noindent\rule[0.5ex]{\linewidth}{1pt}

\textcolor{red}{Ajouter des articles proches par la méthodologie à notre }

- \textcolor{red}{le cas du modèle simple,  }
- \textcolor{red}{le cas du marchéliée, } 
- \textcolor{red}{le cas des clusters.}

\noindent\rule[0.5ex]{\linewidth}{1pt}

## Utilisation des pesticides dans la viticulture

Les phytosanitaires sont très utilisés dans les cultures comme la viticulture. 
Il s’agit donc d’un intrant important pour la production de vin. Ainsi, la viticulture utilisait 15% de produit phytosanitaire. 
La pression sanitaire varie selon les productions et elle est particulierement forte en viticulture. 
De la même façon, la pression phytosanitaire varie selon les régions. 
Ainsi, pour la vigne l’IFT varie de 7 en Provence à 22 en Champagne.

# 3. Le Modèle théorique

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Théorie mod  ###################
#####################################################
```

## Les hypothèses théoriques

\noindent\rule[0.5ex]{\linewidth}{1pt}

\textcolor{red}{Ajouter les réferences ...}

\textcolor{red}{Ajouter la partie sur la demande non rélié ...}

\noindent\rule[0.5ex]{\linewidth}{1pt}

Comme proposé dans la littérature, notre étude sur les vins non coûteux (non IGP) est effectué au niveau du pays @cembalo2014 pour deux raisons. 
D'abord, les prix de vente moyens des marchés sont diffèrent en raison des droits de douane à l'importation et des taxes à la consommation différents (Anderson et Nelgen 2011).
De plus, la perception des produits de consommation varie d'un pays à l'autre (Makela et al. 2006).

La plupart des bouteilles achetées sont achetées dans la grande distribution. 
Néanmoins, dans un souci de simplicité nous estimerons que les consommateurs achètent leurs bouteilles directement auprès du viticulteur. 
Donc nous supprimerons tous les intermédiaires entre le producteur et le marché final.

Quand aux exportations et les importations, n'ayant pas la possibilité contrôler le montant des vins non IGP exportés/importés, nous laissons ces effets au terme d'erreur. 
Nous ignorons les interactions internationales completement. 

Pour conclure, nos suppositions au niveau du marché des vins sont les suivantes :

1. La demande pour les vins simples est unique pour toute la France. On n'observe pas les quantités consommés par départements, mais pour tout le pays, avec un prix unique. 
2. La production du vin varie par département, suite à des différences climatologiques.
3. On n'observe que l'équilibre sur le marché au niveau du pays (la quantité demandé est égale à la quantité offerte par l'ensemble des régions).

En ce qui concerne les pesticides, nous supposons que la demande des pesticides est inélastique au prix, ce qui nous permet d'exclure la partie de l'offre des pesticides du notre analyse. 
C'est-à-dire, la quantité de pesticides utilisée correspond seulement à des intentions et aux besoins des agriculteurs. 

\noindent\rule[0.5ex]{\linewidth}{1pt}

\textcolor{red}{Ajouter une partie sur la clusterisation et les differences du comportement ...}

\textcolor{red}{Ajouter un paragraph sur la comparaison eventuelle de ces modèles et les conclusions possibles ...}

\noindent\rule[0.5ex]{\linewidth}{1pt}

## Formalisation 

\noindent\rule[0.5ex]{\linewidth}{1pt}

\textcolor{red}{Formaliser la partie sur la demande non rélié ...}

\noindent\rule[0.5ex]{\linewidth}{1pt}

En formalisant notre modèle théorique, nous posons, que l'offre agregée pour toute la France est donnée identiquement par l'équation suivante : 

\begin{equation}
    Qo = \sum_{i = 1}^{N} qo_i
\end{equation}

Avec la quantité offerte déterminé par des contraintes de production et le prix sur le marché :

\begin{equation}
    qo_i = a_i + b_i Po_i + c_i X_i
\end{equation}

Où $X$ est un vecteur des variables explicatives influençant la production. Dans le cas le plus simple nous ne prenons en compte que les quantités des pesticides utilisées et la surface disponible, alors l'effet $c_{i1} : c_i = (c_{i1}, c{i2})$ represente l'effet d'utilisation des pésticides dans la production du vin sur l'offre du dérnier.

Cette équation déjà en soit permet d'estimer les effets d'utilisation des pésticides sur le marché du vin (appelons cette méthode M1 pour la réferencier en futur). 
Toutefois, ces résultats ne séront valides que dans la situation où la quantité du vin simple offerte sur le marché est déterminée seulement par le producteur et n'est pas entrélié avec la demande. 
Comme nous avons vu dans la séction précedente, la demande peut influencer les décisions des viticulteurs (ex: le choix de la procedure téchnique à suivre, d'utiliser ou non les pésticides, etc \dots).
Dans le cas pareil, nous dévrions prendre en compte les intéractions entre l'offre et la demande.
A ce but nous introduisons également la demande dans notre analyse. 

La demande agregée du vin en France peut s'écrire sous la forme suivante :

\begin{equation*}
    Qd = \sum_{i = 1}^{N} qd_i 
\end{equation*}

Où $i \in \{1, ..., N\}$ sont des départements, chacun ayant sa propre fonction de la demande unique : 

\begin{equation*}
    qd_i = \alpha_i + \beta_i Pd_i + \gamma_i Z_i 
\end{equation*}

Avec $Z$ étant l'ensemble des variables ayant une influence sur la demande du vin, dans le cas le plus simple nous n'utilisons que les revenus (c'est une des variables les plus utilisées dans des études empiriques sur le marché du vin).

Pour intégrer cette information dans notre *framework* analytique, nous dévons construire une système d'équations.
Il y existe plusieures façons de le faire. 

Dans le prémièr cas, nous pouvons essayer de capter les effets au niveau national.
Pour le faire nous réecrivons les deux équation (de la demande et de l'offre respectivement) sous la forme suivante :

\begin{equation*}
    Q_o = \sum_{i = 1}^{N} (a_i + b_i Po_i + c_i X) = \sum_{i = 1}^{N} a_i + \sum_{i = 1}^{N} b_i Po_i + \sum_{i = 1}^{N} c_i X
\end{equation*}

\begin{equation*}
    Qd = \sum_{i = 1}^{N} ( \alpha_i + \beta_i Pd_i + \gamma_i Z_i ) = \sum_{i = 1}^{N} \alpha_i + \sum_{i = 1}^{N} \beta_i Pd_i + \sum_{i = 1}^{N} \gamma_i Z_i
\end{equation*}

Ce qui nous produira un système des deux équations, avec $Qd = Qo$ dans la situation d'équilibre :

\begin{align*}
    Qd & = \sum_{i = 1}^{N} \alpha_i + \sum_{i = 1}^{N} \beta_i Pd_i + \sum_{i = 1}^{N} \gamma_i Z_i \\
    Qo & = \sum_{i = 1}^{N} a_i + \sum_{i = 1}^{N} b_i Po_i + \sum_{i = 1}^{N} c_i X
\end{align*}

Neanmoins, ce cas se réleve d'être très complex. 
D'abord, les effets peuvent être differents pour tous les départements, ce qui nous conduira à une augmentation dans le nombre des paramètres à estimer significative. 
De plus, même si tous les effets sont identiques pour l'ensemble des départements, des contraintes au niveau des données peuvent se reveler d'être trop restrictives en réduisant au néant la puissance statistique de notre éstimateur (ex : le nombre des observation par années très faible). 
Dans le deux cas nous faisons face à un impace.

Une des modification possibles dans ce cas sera l'introduction d'une contrainte supplementaire au niveau de la demande sur le vin de table.
Afin de pouvoir identifier les effets de toutes les variables par un système d'équations, nous pouvons supposer, que tout le vin produit dans un département est consommé dans le même department. 
Dans ce cas nous pourrions obtenir des estimateurrs pour les effets moyens au niveau départemental.

Toutefois, c'est une supposition forte, laquelle nous éloigne de la réalité. 
De cette façon nous ignorons plusieurs effets pervers, tels que :

- La structure du marché interne de la France ;
- La mobilité des produits finis entre des differents départements ;
- L'exportation et l'importation du vin.

Théoriquement, nous pouvons tout de méme ignorer ces effets, car nous visons à estimer les effets moyens pour tous les départements. 
De cette façon, lors d'aggregation des effets au niveau national en éstimant le coefficient moyen unique pour tous les départements nous allons mitiger les biais possibles.

Alors,nous pouvons réécrire notre système d'equations sous la forme suivante :

\begin{align*}
  qd_i & = \alpha_{i} + \beta Pd_{i,d} + \gamma Z_{i} \\
  qo_i & = a_i + b Po_{i,o} + c X_{i} \\ 
\end{align*}

Où $qd_i = qo_i$ et $Pd_i = Po_i$, ce qui permet de rélier les équations au niveau départemental.
Les coefficients $b$, $c$, $\beta$ et $\gamma$ sont supposé fixes pour tous les départements nous donnent un estimateur des effets moyens au niveau de la France.
L'effet des pésticides dans la production du vin serons captés par le terme $c_{1} : c = (c_{1}, c{2})$ dans ce cas.

Néanmoins, nous nous posons la question, comment réagir dans le cas où les effets sont differents pour des differents département suite à des spécificité des marché locaux ?
On peut supposer, qu'il existent au moins quelques groupes majeures ayant des caractéristiques et comportements similaires. 
Dans ce cas nous pourrions construire des clusters, qui regrouppont des départements ayant des caractéristiques idéntiques. 
Cela nous permettra de modèliser les effets moyens par cluster en réduisant les biais eventuels.

Ce système peut être formalisé par $K$ systèmes d'équations suivantes :

\begin{align*}
  qd_i_{j = const} & = \alpha_{i_{j = const}} + \beta_{j = const} Pd_{i_{j = const},d} + \gamma_{j = const} Z_{i_{j = const}} \\
  qo_i_{j = const} & = a_i_{j = const} + b_{j = const} Po_{i_{j = const},o} + c_{j = const} X_{i_{j = const}} \\ 
\end{align*}

Où $j$ décrive l'appartenance des départements à un des clusters.

\noindent\rule[0.5ex]{\linewidth}{1pt}

\textcolor{red}{Formaliser la partie sur la clusterisation et les differences du comportement ...}

\textcolor{red}{Formaliser le paragraph sur la comparaison eventuelle de ces modèles et les conclusions possibles ...}

\noindent\rule[0.5ex]{\linewidth}{1pt}

# 4. Les données

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Données mob  ###################
#####################################################
```

Dans cette partie de notre travail nous allons presenter la base des données utilisé lors de cette étude. 
Nous commencerons par une presentation des sources et des types des données extraits de ces sources. 
Puis, nous procederons avec la déscription des méthodes et thécniques utilisées pour transformer ces données et les rendre traitables. 
Finalement, nous presenterons un dictionnaire des variables pour nos bases des données.

## Sources des données : 

Nous avons utilisé les bases des données suivantes pour notre analyse :

- Les données de ventes de pesticides par département (INERIS)
- Les données sur les prix du vin (France Agrimer)
- Les données sur la population (INSEE)
- Les données sur la production de vin (SSM Finances Publiques)

## Les variables utilisées pour notre modèle

\noindent\rule[0.5ex]{\linewidth}{1pt}

\textcolor{red}{Réverifier tous les sources et la naure des données ...}

\textcolor{red}{Expliciter la procedure de création des variables}

\textcolor{red}{Preciser les effets attendus des variables}

\textcolor{red}{Discuter les externalités (ou c'est mieux de l'inclure dans la partie théorique ? ou contextualisation ? A VOIR)}

\noindent\rule[0.5ex]{\linewidth}{1pt}

Dans notre étude nous faisons face à un problème avec deux variables endogènes et trois variables exogènes.

Variables endogènes : 
- la quantité totale produite de vin rouge et blanc non IG par département (en hectolitres, en log), 
- le prix moyen des vins rouges-blancs (idice, en log).

Variables exogènes : 
- le revenu médian par département (en euros par personne par année, en log), 
- la surface agricole destinée aux vins de table (en hectares, en log),
- la quantité des pesticides utilisés sur la vigne (indice, en log).

Au niveau des pesticides, on va s’intéresser plus particulièrement aux quantités de produits vendus par département entre 2009 et 2017 utilisés principalement sur les cultures viticoles. 
Il faut faire preuve de vigilance sur le conditionnement des produits qui n’est pas exprimé dans la même unité au sein de cette base : en litres ou en kilos.
Dans notre étude nous allons étudier l'impact de la masse totale des pésticides utilisés.
Pour pouvoir le faire, nous créons un indice qui permet de prendre en compte les évolutions des differents types des produits à la fois.
Nous créons un indice simple :

\begin{equation*}
  P = \frac{\sum_j p_{j, t} q_{j, t}}{\sum_j p_{j, 0} q_{j, 0}}
\end{equation*}

Avec $j$ désignant le produit $j$, et $p$ étant un coefficient de pondération (dans le cas le plus simple $p = 1$).

En ce qui concerne les données sur le prix du vin, on s’intéresse principalement au prix moyen des vins rouge- rosés et blancs sans IG (Indication Géographique) sur la période 2009-2017. 
Ces prix sont déflatés par l’indice des prix à la consommation (base 100 en 2014). 
On ne considère ici que le prix moyen déflaté au niveau national.
Dans le deuxième modèle nous avons besoin de créer artificiellement un estimateur qui va varier par département.
Dans ce but nous créons l'indice de prix du vin de table départementale, calculé de façon suivante :

\begin{equation*}
  P = \frac{p_{rouge, t} q_{rouge, t} + p_{blanc, t} q_{blanc, t}}{p_{rouge, 0} q_{rouge, 0} + p_{blanc, 0} q_{blanc, 0}}
\end{equation*}

Avec $t$ étant l'anée au période $t$.

Au niveau des données sur la population, la variable qui nous intéresse ici est relative au niveau de revenu, exprimée au niveau départemental (laquelle, si besoin nous pourrions facilement aggréger au niceau national). 
Plus précisément, on va utiliser le revenu médian par département.
Il est aussi déflatée de l’indice des prix à la consommation (base 100 en 2014).

Toutes les variables subissent une transformation logarithmique, ce qui nous permet d'interpreter les effets estimés plus facilement. 
Pour un modèle logarithmique nous pourrions traiter les estimateurs obtenus comme l'elasticité de la demande/l'offre par rapport à des facteurs differents. 
Ainsi, nous cherchons particulierement l'elasticité de quantité offerte sur le marché par rapport à la quantité des pésticides utilisés.

Les propriétés de ces données sont suivantes :

- Toutes les variables varient par département et par année.
- Le période temporelle comprise dans notre échantillon est de 2012 à 2016.
- Nous ne considérons que les régions produisant du vin. 
- Nous éliminons les effets fixes pour en substrayant les moyennes départamentales.
- Données en panel "cylindrées".
- Nombre des individus large (69 départements, qui produisent le vin et utilisent des pésticides) et le nombre des périodes pauvre (5 périodes).

# 5. L'étude statistique

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Statistique  ###################
#####################################################
```

Dans cette partie de travail nous allons explorer les données collectées. 

De l'étude de la variance pour les données en panel avec des statistiques générales, nous passerons vers l'étude des interdependances des variables. 
Pui, nous allons finir avec étude des donnée alternées par une transformation **Within**.

## Visualisation au niveau de la France 

Pour le prémier analyse il peut être interessant de voir la situation du point du vue géographique. 
Nous visualisons les valuers moyens par département des differentes variables.

```{r include = FALSE}
# Creation de la base des données 
SpData = datai %>% 
    group_by(ndep) %>%
    summarise_all(mean) %>%
    dplyr::select(ndep, qi, ipi, iki, ri, si)
# Formes
formes = getData(name = "GADM", country = "FRA", level = 2)
plot(formes, main = "Carte de la France, départements")
# Index 
ind = match(as.numeric(formes$CC_2), SpData$ndep)
# Data transfer
qisp = SpData[ind, "qi"]
ipisp = SpData[ind, "ipi"]
ikisp = SpData[ind, "iki"]
sisp = SpData[ind, "si"]
risp = SpData[ind, "ri"]
# Writting 
formes$qi = qisp
formes$ipi = ipisp 
formes$iki = ikisp 
formes$ri = risp 
formes$si = sisp 
```

D'abord nous étudions le comportement de la variable dépendante de notre systhéme. 
La quantité du vin sans IG produit par département semble pouvoir être correlé.

\FloatBarrier

```{r echo = FALSE, results = "asis", fig.cap = "Les quantité du vin non-IG moyennes par département"}
# Plot 
spplot(formes, "qi", 
    col.regions = colorRampPalette(c('grey96', 'red'))(30),  
    main = list(label = "Quantité du vin produite par département", cex = 0.8))
```

\FloatBarrier

Puis, nous observons le comportement du reste des variables ...

L'indice des prix se comporte pratiquement comme quantité du vin produite, car cet indice fut construit par biaisd e cette variable. 

L'indice décrivant la quantité des pésticides utilisés ...

## Etude de la variance 

Passons maintenant à l'étude de la variance. 
Nous allons décortiquer la variance par type (between et within) afin d'obtenir une idée sur le choix preferable de la dimention d'aggregation des nos données.

```{r include = FALSE}
lxtsums = list()
# list
lxtsums[[1]] = xtsum(datai, ipi, ndep)
lxtsums[[2]] = xtsum(datai, iki, ndep)
lxtsums[[3]] = xtsum(datai, si, ndep)
lxtsums[[4]] = xtsum(datai, ri, ndep)
lxtsums[[5]] = xtsum(datai, t, ndep)
# results
results = print.xtsum(lxtsums)
rownames(results) = c("Index prix", "Index pesticides",
    "Surface", "Revenus", "Temps")
```

Le tableau suivant regrouppe les statistiques déscriptives essentielles : 

- Moyennes 
- Variance sur l'échantillon complet 
- Variance *between* 
- Variance *within*

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(results,  
    header = FALSE,
    title = "Etude de la variance",
    summary = FALSE)
```

\FloatBarrier

Il est facile a rémarquer que la variance *between* est plus significative que la variance *within*. 
Cela nous amêne à l'idée qu'il faut utiliser un modèle qui permettra estimer et corriger ces inégalités entre les individus, car nous sommes plus interessés par des effets individuels moyens (les effets moyens pour tous les individus).
Ce qui est completement conforme à notre hypothèse qu'on a exprimé lors de la formalisation du modèle économique théorique.

```{r include = FALSE}
Formulas = list(
    ipi = qi ~ ipi,
    iki = qi ~ iki,
    si = qi ~ si,
    ri = qi ~ ri)
Dtest = Effect.testing(Formulas, data = datap)
rownames(Dtest) = c("Index prix", "Index pesticides",
    "Surface", "Revenus")
```

De plus, il est interessant d'observer les résultats obtenus pour le test de Chow comparant le modèle complet (*pooled model*) contre les modèles au effet fixes et randomes. 
Le tableau suivant régrouppe les p-valeurs de ce test pour les modèles univariées differents.

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(Dtest[,c(1:2)],  
    header = FALSE,
    title = "Les p-valeurs de pooling-test de Chow",
    summary = FALSE)
```

\FloatBarrier

Sauf le cas de la surface nous ne pouvons pas rejeter l'hypothese nulle, specifiant que les individus ont des effets identiques pour toute la population. 

## L'étude des types d'effets  

\noindent\rule[0.5ex]{\linewidth}{1pt} 

\textcolor{red}{Expliciter pourquoi on choisis les effets fixes (une comparaison avec les effets randomisés ?)}

\noindent\rule[0.5ex]{\linewidth}{1pt}

Nous avons déjà vu, qu'il est fortement probable que nous faisons face à un modèle aux effets fixes individuelles. 
Il faut quand même le justifier.
Pour faire cela, nous allons effectuer le test de multiplicateur de Lagrange sur la nature des effets fixes. 
Selon les résultats il est évidente que nous avons des effets fixes au niveau individuel pour toutes les variables. 
Pour la variable de surface nous avons des effets à la fois individuels et temporels. 

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(Dtest[,c(3:ncol(Dtest))], 
    header = FALSE,
    title = "p-valeurs de Lagrange multiplier test",
    summary = FALSE)
```

\FloatBarrier

## L'analyse de la correlation

Maintenat nous allons ... 

## La transformation **within**

```{r include = FALSE}
###################
# Rework the matrix WITHIN_TRANSFORM
###################
rm(datax) ; rm(datay) ; rm(data) ; rm(dn)
dataW = datap 
dataW$qi = Within(datap$qi)
dataW$ipi = Within(datap$ipi)
dataW$iki = Within(datap$iki)
dataW$si = Within(datap$si)
dataW$ri = Within(datap$ri)
```

# 6. Modèlisation

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Modèlisation  ##################
#####################################################
```

\noindent\rule[0.5ex]{\linewidth}{1pt} 

\textcolor{red}{Séparer les modèles (OLS, 3SLS avec justification par 2SLS et la comparaison avec i3SLS, clusters en OLS et 3SLS).}

\textcolor{red}{Justifier le choix des modèles par 3 cas théoriques. Discuter les avantages et les inconveniences}

\textcolor{red}{Ajouter des liens avec des études méthodologiques precedents.}

\textcolor{red}{Pour le modèle 2SLS préciser la forme, tester les instruments}

\textcolor{red}{Arbitrage du choix de 2SLS vs 3SLS}

\noindent\rule[0.5ex]{\linewidth}{1pt}

Cette partie du travail abordera la formulation économétrique du notre problème.
Nous allons débuter par la présentation des notions théoriques implimentés dans ce travail, suivis par la formalisation économétrique du modèle théorique que nous avons spécifié dans la séction 5.
Après, nous expliquerons la stratégie d'identification utilisée.

## Presentation de la méthodologie

L'AIDS (*almost ideal demand system*) et les autres modèles de demande cités dans la littérature ont de nombreuses lacunes qui les rendent impropres pour l'estimation du marché du vin, selon @cembalo2014. 
Quand même, dans notre étude nous allons utiliser exactement ce modèle là, sous des suppositions restrictives. 
Ces hypothèses nous permettent d'utiliser le modèle de type AIDS sur un segment détaché du marché de vin, qui regrouppe les vins de table. 

Ce modèle nous permettra de simuler l'équilibre sur le marché du vin, prenant ainsi en compte la pluspart des facteurs incitant les producteurs du vin d'utiliser les pésticides. 

## Modèle économétrique 

Le modèle mise en place vise à estimer les effets moyenns pour tous les départements. 
De cette façon, l'aggregation des effets au niveau national nous permet de mitiger les biais eventuels, liés à la misspecification du modèle.

Nous pouvons réécrire notre système d'equations sous la forme suivante :

\begin{align*}
  qd_{1,t} & = \alpha_{1} + \beta Pd_{1,t} + \gamma Z_{1,t} + \epsilon_{1,t}  \\
  \vdots \\ 
  qd_{N,t} & = \alpha_{N} + \beta Pd_{N,t} + \gamma Z_{N,t} + \epsilon_{1,t}  \\
  qo_{1,t} & = a_1 + b Po_{1,t} + c X_{1,t} + u_{1,t} \\ 
  \vdots \\ 
  qo_{N,t} & = a_N + b Po_{N,t} + c X_{N,t} + u_{N,t} \\
\end{align*}

Nous posons que l'offre et la demande sont egaux au niveau de département : $qd_{i,t} = qo_{i,t}$.
C'est à dire l'offre interne du département vise à satisfaire la demande interne du même département. 

En termes d'aggregation ex-post des effets estimés, nous sommes sensé de tomber sur l'équilibre au niveau du marché national. 
En d'autre mots, le système :

\begin{align*}
  qd_{1,t} & = qo_{1,t} \\
  \vdots \\ 
  qd_{N,t} & = qo_{N,t} \\
\end{align*}

Implique : $Qd = Qo$.

Au point d'équilibre nous avons également l'égalité des prix :

\begin{equation*}
  Po_{1,t} = Pd_{1,t}
\end{equation*}

De cette façon nous obtenons un système des systèmes des équations.
En simplifiant l'écriture nous pouvons la representer sous la forme suivante :

\begin{align*}
  q_{i,t} & = \alpha_{i} + \beta P_{i,t} + \gamma Z_{i,t} + \epsilon_{i,t} \\
  q_{i,t} & = a_i + b P_{i,t} + c X_{i,t} + u_{i,t}
\end{align*}

D'ici nous avons à notre disposition plusieurs chemins differents à traiter ce modèle du point de vue économétrique. 
Le plus simple est d'estimer les effets des pésticides sur l'offre du vin en ignorant les impacts du comportement des consommaterus sur les producteurs. 
Cette méthode implique une éstimation par OLS simples (ou WLS et SUR, lesquels introduisent des poids aux équations ou permettent de traiter la correlation entre les résidus).

D'autre coté, nous pouvons implementer les doubles ou tripples moindre carrés (2SLS, W2SLS et 3SLS), qui nous permettrons d'obtenir des résultats identique aux résultats d'estimation des équation structurelles. 
Cette méthode offre la possibilité d'estimer le système d'équations avec plusieurs variables endogèenes en prenant en compte les deux coté du marché à la fois. 

Finalement, si on trouve qu'il y existe une heterogeneité entre les départements en termes d'équilibre interne, nous pourrions réestimer le modèle en clusterisant nos *individus* (départements) par des differents classes selon leurs attributs, pour après estimer les equations par cluster.

```{r include = FALSE, eval = FALSE}
###################
# Not evaluated !!!
###################
###################
# Rework the matrix DUMMIES
###################
Dum = dummy(datai$ndep, sep = "_")
# Index pésticides
IKI = as.matrix(datai$iki)[, rep(1, each = length(unique(datai$ndep)))]
ikiDum = as.data.frame(Dum*IKI) %>% 
    setNames(paste0('iki_', names(.)))
rm(IKI)
# Revenus
RI = as.matrix(datai$ri)[, rep(1, each = length(unique(datai$ndep)))]
riDum = as.data.frame(Dum*RI) %>% 
    setNames(paste0('ri_', names(.)))
rm(RI)
# Concatenate
dataD = datai %>% 
    dplyr::select(-c(t, ri, iki)) %>% 
    cbind(Dum) %>% 
    cbind(ikiDum) %>% 
    cbind(riDum)
```

```{r include = FALSE, eval = FALSE}
###################
# Not evaluated !!!
###################
###################
# Rework the matrix WITHINxDUMMIES
###################
Dum = dummy(datai$ndep, sep = "_")
# Index pésticides
IKI = as.data.frame(dataW$iki, ncol = 1)[, rep(1, each = length(unique(datai$ndep)))]
ikiDum = as.data.frame(Dum*IKI)
ikiDum = ikiDum %>%
    set_names(~str_replace_all(., "dataW\\$", ""))
rm(IKI)
# Revenus
RI = as.data.frame(dataW$ri)[, rep(1, each = length(unique(datai$ndep)))]
riDum = as.data.frame(Dum*RI) %>%
    rename_all(list(~str_replace_all(., "dataW\\$", "")))
rm(RI)
# Concatenate
dataWD = dataW %>% 
    dplyr::select(-c(t, ri, iki)) %>% 
    cbind(ikiDum) %>% 
    cbind(riDum)
rm(ikiDum) ; rm(riDum)
```

## La sratégie d'identification 

Nous attendons à ce que l'éstimateur de 3SLS, qui permet de capter tous les effets de correlations entre les équation en presence de plusieures variables exogènes nous permettra d'obtenir des estimations les plus fiables. 
Cette méthode nous permet à depasser le biais de simultanéité qui apparaisse dans le cas d'estimation dessystèmes d'équations.
De plus, cet estimateur (3SLS) donne des résultats similaires à l'éstimateur de ILS (*indirect least squares*).

Les propriétés de cet éstimateurs sont :

- Consistence ;
- Efficience (asymptotique) ;
- La distribuitions pour les estimateurs suit une loi normale suelement dans des grands échantillons.

Quand même dès le debut nous envisageons des problèmes possibles avec les résultats obtenus. 
Parmis lesquels on a la faible representation des effets hetérogenes entre les départements (nous estimons seulemnt les effets moyens et ainsi ignorons les differences des élasticités pour des départements differents).
De plus, nous ignorons la présence d'autocorrelation spatiale et/ou temporelle dans notre modèle. 
Finalement, un nombre insuffisant des facteurs est utilisé dans ce modèle, ce qui risaue d'apporter le biais des variables omises dans nos estimations. 

# 7. Résultats des estimations 

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Modèlisation  ##################
#####################################################
```

Dans cette séction nous allons presenter les prémiers résultats économétriques.
Lors de ces prémiéres éstimations nous supposons que les effets sont identiques pour tous les département (on fait la correction pour les effets fixes au niveau départemental quand même).

```{r eval = FALSE, include = FALSE}
# Nous allons presenter en particulier :

# - Les coefficients estimés avec leurs variance 
# - L'efficience et comparaison des estimateurs 
# - Etude des erreurs 
#     - La distribution des erreurs
#         - La normalité 
#         - Centrage sur 0 
#         - Independance des variables explicatives 

#     - L'autocorrelation des résidus 
#     - L'hétéroskedacité  
```

Nous estimons un enseble des differents modèles possibles afin de pouvoir choisir la méthode la plus raisonnable. 
Les modèles suivantes sont traitées séparement :

- Modèles simples : OLS, WLS et SUR (ou nous concentrons notre attention sur l'équation d'offre du vin)
- Modèlesdes équations simultanées avec des variables endogènes : 2SLS, W2SLS, 3SLS et 3SLS itérés (le dérnier modèle étant similaire à FIML (*full information maximum likelihood*))

```{r include = FALSE, eval = FALSE}
# - Vérification des hypothèses (5 hypothèses) :
#     - La moyenne nulle des erreurs 
#     - La normalité des residus 
#     - Homoscedacité 
#     - Autocorrélation 
#     - Spécification du modèle
# 3SLS and FIML are asymptotically equivalent. 
# Hence 3SLS is efficient and FIML is consistent even if residuals are not normal.
```

## Les résultats OLS, WLS et SUR

```{r include = FALSE}
# Data transformation for systemfit
dataWX = as.data.frame(dataW)
```

```{r include = FALSE}
# equations
eqdemand = qi ~ 0 + ipi + ri
eqoffer = qi ~ 0 + ipi + si + iki 
system = list(Demande = eqdemand, Offre = eqoffer)
# OLS
ols = systemfit(system, 
    data = dataWX, 
    method = "OLS")
# WLS
wls = systemfit(system, 
    data = dataWX, 
    method = "WLS")
# SUR
sur = systemfit(system, 
    data = dataWX, 
    method = "SUR")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
texreg(float.pos = "!htbp", 
    list(ols, wls, sur),
    custom.model.names = c("OLS", "WLS", "SUR"),
    label = "table : ols, wls and sur")
```

\FloatBarrier

## Les résultats 2SLS, W2SLS, 3SLS et i3SLS

\FloatBarrier

```{r include = FALSE}
# equations
eqdemand = qi ~ 0 + ipi + ri
eqoffer = qi ~ 0 + ipi + si + iki 
inst = ~ ri + si + iki
system = list(Demande = eqdemand, Offre = eqoffer)
# 2SLS
# 2SLS is an equivalent of ILS (indirect least squares)
sls2 = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "2SLS")
# 2WSLS
wsls2 = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "W2SLS")
# 3SLS (errors correction)
sls3 = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "3SLS")
# FIML (iterated 3SLS)
fiml = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "3SLS", maxit = 1000)
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
texreg(float.pos = "!htbp", 
    list(sls2, wsls2, sls3, fiml),
    custom.model.names = c("2SLS", "W2SLS", "3SLS", "i3SLS"),
    label = "table : 2sls, w2sls, 3sls and fiml")
```

\FloatBarrier

## Clusterisation et modèlisation par groupe 

### *Between* 

Nous avons vus dans le comportement des résidus une nature non-aléatoire grouppé. 
Cela nous amène à l'idée de construire k-clusters pour modèliser les rélations par grouppe.

Nous supposons que les départements ayant des valeurs moyennes interannuelles proches (transformation Between) ont le comportement identique.
La clusterisation est effectué sur les données Between pour les départements.

```{r include = FALSE}
# Clustering 
# Between dataframe
dataB = datap 
dataB$qi = Between(datap$qi)
dataB$ipi = Between(datap$ipi)
dataB$iki = Between(datap$iki)
dataB$si = Between(datap$si)
dataB$ri = Between(datap$ri)
dataB$ndep = index(datap)$ndep
# Between correction
dataB = dataB %>% 
    dplyr::select(-t)
dataB = dataB %>% 
    group_by(ndep) %>% 
    summarise_all(mean)
# Analysis for clustering
wss = (nrow(dataB[,-1])-1)*sum(apply(dataB[,-1], 2, var))
for (i in 2:15) {
    wss[i] = sum(kmeans(iter.max = 100, dataB[,-1], centers = i)$withinss)
}
```

Nous povons supposer que le nombre des clusters optimal est entre 3 et 5.
Prenant en compte les graphiques des résidus vus lros d'analse des modèles nous allons supposer qu'il n'y a que 3 clusters principaux.

\FloatBarrier

```{r echo = FALSE, fig.cap = "Le choix des clusters"}
plot(1:15, wss, type = "l", 
    xlab = "Nomber de clusters",
    ylab = "WSS") # 3, 4, 5
points(1:15, wss, col = "red")
```

\FloatBarrier

```{r include = FALSE}
dataWi = as.data.frame(dataW) 
dataWi$ndep = as.factor(index(dataW)$ndep)
```

```{r include = FALSE}
# Grouping
fit1 = kmeans(iter.max = 100, dataB[,-1], 3)
# Sup 

nclef = data.frame(ndep = dataB$ndep, clust = fit1$cluster)
dataWY = left_join(dataWi, nclef, by = "ndep")
```

### *Within*

Nous avons vus dans le comportement des résidus une nature non-aléatoire grouppé. 
Cela nous amène à l'idée de construire k-clusters pour modèliser les rélations par grouppe.

D'abord on compare le comportement des cluster pour les données à l'information complete et les données Within.

Comme nous pouvons voir dans les résultats le nombre des cluster optimaux est trop large pour les séparer dans l'analyse :

```{r include = FALSE}
# Clustering 
# Data creation
dataClust = dataWi
dataClust_t1 = dataClust %>% 
    filter(t == 1) %>% 
    dplyr::select(-t)
dataClust_t2 = dataClust %>% 
    filter(t == 2) %>% 
    dplyr::select(-t)
dataClust_t3 = dataClust %>% 
    filter(t == 3) %>% 
    dplyr::select(-t)
dataClust_t4 = dataClust %>% 
    filter(t == 4) %>% 
    dplyr::select(-t)
dataClust_t5 = dataClust %>% 
    filter(t == 5) %>% 
    dplyr::select(-t)
```

```{r include = FALSE}
# Joining data
dataC = inner_join(dataClust_t1, dataClust_t2, 
    by = "ndep", suffix = c("", ".t2")) 
dataC = inner_join(dataC, dataClust_t3, 
    by = "ndep", suffix = c("", ".t3")) 
dataC = inner_join(dataC, dataClust_t4, 
    by = "ndep", suffix = c("", ".t4")) 
dataC = inner_join(dataC, dataClust_t5, 
    by = "ndep", suffix = c("", ".t5")) 
```

```{r include = FALSE}
# Analysis for clustering
wss = (nrow(dataC[,-6])-1)*sum(apply(dataC[,-6], 2, var))
for (i in 2:15) {
    wss[i] = sum(kmeans(iter.max = 100, dataC[,-6], centers = i)$withinss)
}
```

Nous povons supposer que le nombre des clusters optimal est entre 6 et 15.

\FloatBarrier

```{r echo = FALSE, results = "asis"}
plot(1:15, wss, type = "l", 
    xlab = "Nomber de clusters",
    ylab = "WSS") # 3, 4, 5
points(1:15, wss, col = "red")
```

\FloatBarrier

```{r include = FALSE}
# Grouping
fit2 = kmeans(iter.max = 100, dataC[,-6], 6)
# require(FactoMineR)
nclef2 = data.frame(ndep = as.factor(dataC$ndep), clust = fit2$cluster)
dataWZ = left_join(dataWi, nclef2, by = "ndep")
clusters2 = cbind(fit2$centers, fit2$size, k <- c(1:6)) %>%
    as.data.frame()
```

### Information complete

Dans le cas d'information complete on a :

```{r include = FALSE}
# Clustering 
# Data creation
dataClustx = datai 
dataClustx_t1 = dataClustx %>% 
    filter(t == 1) %>% 
    dplyr::select(-t, -year)
dataClustx_t2 = dataClustx %>% 
    filter(t == 2) %>% 
    dplyr::select(-t, -year)
dataClustx_t3 = dataClustx %>% 
    filter(t == 3) %>% 
    dplyr::select(-t, -year)
dataClustx_t4 = dataClustx %>% 
    filter(t == 4) %>% 
    dplyr::select(-t, -year)
dataClustx_t5 = dataClustx %>% 
    filter(t == 5) %>% 
    dplyr::select(-t, -year)
```

```{r include = FALSE}
# Joining data
dataCx = inner_join(dataClustx_t1, dataClustx_t2, 
    by = "ndep", suffix = c("", ".t2")) 
dataCx = inner_join(dataCx, dataClustx_t3, 
    by = "ndep", suffix = c("", ".t3")) 
dataCx = inner_join(dataCx, dataClustx_t4, 
    by = "ndep", suffix = c("", ".t4")) 
dataCx = inner_join(dataCx, dataClustx_t5, 
    by = "ndep", suffix = c("", ".t5")) 
```

```{r include = FALSE}
# Analysis for clustering
wss = (nrow(dataCx[,-1])-1)*sum(apply(dataCx[,-1], 2, var))
for (i in 2:15) {
    wss[i] = sum(kmeans(iter.max = 100, dataCx[,-1], centers = i)$withinss)
}
```

Nous povons supposer que le nombre des clusters optimal est entre 3 et 5.
Prenant en compte les graphiques des résidus vus lros d'analse des modèles nous allons supposer qu'il n'y a que 3 clusters principaux.

\FloatBarrier

```{r echo = FALSE, results = "asis"}
plot(1:15, wss, type = "l", 
    xlab = "Nomber de clusters",
    ylab = "WSS") # 3, 4, 5
points(1:15, wss, col = "red")
```

\FloatBarrier

```{r include = FALSE}
# Grouping
fit3 = kmeans(iter.max = 100, dataC[,-6], 3)
# require(FactoMineR)
nclef3 = data.frame(ndep = as.factor(dataC$ndep), clust = fit3$cluster)
dataWZ = left_join(dataWi, nclef3, by = "ndep")
clusters3 = cbind(fit3$centers, fit3$size, k <- c(1:3)) %>%
    as.data.frame()
```

### Modèlisation 

\FloatBarrier

```{r include = FALSE}
# Data transformation
dataWZ = dataWZ %>% 
    mutate(ipi1 = ipi*as.numeric(clust == 1),
        ipi2 = ipi*as.numeric(clust == 2),
        ipi3 = ipi*as.numeric(clust == 3),
        ri1 = ri*as.numeric(clust == 1),
        ri2 = ri*as.numeric(clust == 2),
        ri3 = ri*as.numeric(clust == 3),
        si1 = si*as.numeric(clust == 1),
        si2 = si*as.numeric(clust == 2),
        si3 = si*as.numeric(clust == 3),
        iki1 = iki*as.numeric(clust == 1),
        iki2 = iki*as.numeric(clust == 2),
        iki3 = iki*as.numeric(clust == 3))
```

```{r include = FALSE}
# Equations
eqdemandz = qi ~ 0 + ipi +
    ri1 + ri2 + ri3 
eqofferz = qi ~ 0 + ipi + 
    si1 + si2 + si3 +
    iki1 + iki2 + iki3
instz = ~ ri1 + ri2 + ri3 + 
    si1 + si2 + si3 +
    iki1 + iki2 + iki3
systemz = list(Demande = eqdemandz, Offre = eqofferz)
```

Nous evaluons le système en introduisant les variables de grouppe (dummy variables) sous l'hypothèse des résidus joints.

```{r include = FALSE}
# OLS
olsx = systemfit(systemz, 
    data = dataWZ, 
    method = "OLS")
# 2SLS
sls2x = systemfit(systemz, 
    inst = instz,
    data = dataWZ, 
    method = "2SLS")
# 3SLS
sls3x = systemfit(systemz, 
    inst = instz,
    data = dataWZ, 
    method = "3SLS")
```

Les résultats obtenus sont suivants : 

\FloatBarrier

```{r echo = FALSE, results = "asis"}
texreg(float.pos = "!htbp", 
    list(olsx, sls2x, sls3x),
    custom.model.names = c("OLS", "2SLS", "3SLS"),
    label = "table : ols, 2sls et 3sls, full information clusters")
```

\FloatBarrier

# 9. Conclusions 

```{r eval = FALSE, include = FALSE}
#####################################################
###################  Modèlisation  ##################
#####################################################
```

- Le marché du vin
- Le rôle des pésticides  
- Validité 

\FloatBarrier

## Le marché du vin
- Un comportement inattendus 
    - Les effets de substitution contre les produits de la haute gamme 
    - Les effets négatives du revenu 
    - 

\FloatBarrier

## Le rôle des pésticides

- Confirmation des résultats des études précedentes 
    - Utilisés pour réduire les pertes 

\FloatBarrier

## Validité 

- Faible validité du modèle économétrique 
    - Variables ommises 

\FloatBarrier

\newpage

# Annexes 

```{r eval = FALSE, include = FALSE}
################################################
###################  Annexes  ##################
################################################
```

## A Les statistiques déscriptives 

### A1 Les moyennes par département 

\FloatBarrier

```{r echo = FALSE, results = "asis", fig.cap = "Les valeurs moyennes par département, partie 1"}
# Plot
p1 = spplot(formes, "ipi", 
    col.regions = colorRampPalette(c('grey96', 'blue'))(30),  
    main = list(label = "Index prix du vin par département", cex = 0.8))
p2 = spplot(formes, "iki", 
    col.regions = colorRampPalette(c('grey96', 'green4'))(30),  
    main = list(label = "Index pesticides par département", cex = 0.8))
grid.arrange(p1, p2, nrow = 1)
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, results = "asis", fig.cap = "Les valeurs moyennes par département, partie 2"}
# Plot
p1 = spplot(formes, "si", 
    col.regions = colorRampPalette(c('grey96', 'red'))(30),  
    main = list(label = "Surface qultivé par département", cex = 0.8))
p2 = spplot(formes, "ri", 
    col.regions = colorRampPalette(c('grey96', 'orange'))(30),  
    main = list(label = "Revenus par département", cex = 0.8))
grid.arrange(p1, p2, nrow = 1)
```

\FloatBarrier

\newpage

### A2 Les graphiques bivariés

#### Cas général 

\FloatBarrier

```{r echo = FALSE, fig.cap = "L'étude bivarié, partie 1"}
p1 = datai %>% 
    ggplot(aes(qi, ipi, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    # ggtitle("Q ~ IP") +
    xlab("Quantité du vin") + ylab("Index du prix") +
    pres_theme
p2 = datai %>% 
    ggplot(aes(qi, iki, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    # ggtitle("Q ~ IK") +
    xlab("Quantité du vin") + ylab("Index des pésticides") +
    pres_theme
grid.arrange(p1, p2, nrow = 1)
# Probably it is better to do it by variable
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, fig.cap = "L'étude bivarié, partie 2"}
p1 = datai %>% 
    ggplot(aes(qi, si, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    # ggtitle("Q ~ S") +
    xlab("Quantité du vin") + ylab("Surface qultivé") +
    pres_theme
p2 = datai %>% 
    ggplot(aes(qi, ri, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    # ggtitle("Q ~ R") +
    xlab("Quantité du vin") + ylab("Revenus") +
    pres_theme
grid.arrange(p1, p2, nrow = 1)
```

\FloatBarrier

#### Transformation *Within*

\FloatBarrier

```{r echo = FALSE, fig.cap = "Rélations bivariés dans le cas de transformation within, partie 1"}
p1 = dataWi %>% 
    ggplot(aes(qi, ipi, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    geom_smooth(aes(qi, ipi, col = "black"), method = "loess", size = 0.4) +
    # ggtitle("Q ~ IP") +
    xlab("Quantité du vin") + ylab("Index du prix") +
    pres_theme
p2 = dataWi %>% 
    ggplot(aes(qi, iki, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    geom_smooth(aes(qi, iki, col = "black"), method = "loess", size = 0.4) +
    # ggtitle("Q ~ IK") +
    xlab("Quantité du vin") + ylab("Index des pésticides") +
    pres_theme
grid.arrange(p1, p2, nrow = 1)
# Probably it is better to do it by variable
```

\FloatBarrier

```{r echo = FALSE, fig.cap = "Rélations bivariés dans le cas de transformation within, partie 2"}
p1 = dataWi %>% 
    ggplot(aes(qi, si, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    geom_smooth(aes(qi, si, col = "black"), method = "loess", size = 0.4) +
    # ggtitle("Q ~ S") +
    xlab("Quantité du vin") + ylab("Surface qultivé") +
    pres_theme
p2 = dataWi %>% 
    ggplot(aes(qi, ri, col = as.factor(ndep))) +
    geom_point(size = 0.5) + 
    geom_smooth(method = "lm", se = F, size = 0.25) +
    geom_smooth(aes(qi, ri, col = "black"), method = "loess", size = 0.4) +
    # ggtitle("Q ~ R") +
    xlab("Quantité du vin") + ylab("Revenus") +
    pres_theme
grid.arrange(p1, p2, nrow = 1)
```

\FloatBarrier

\newpage

### A3 La correlation 

#### Cas général 

Le premier tableau combrend les résultats pour les données telles-quelles, le deuxieme par contre integre les résultats pour les données sous la trasformation *within*.

```{r include = FALSE}
correlation = cor(datap)
colnames(correlation) = c("Quantité du vin", "IP", 
        "Surface", "Revenus", 
        "Index pésticides", "Temps")
rownames(correlation) = c("Quantité du vin", "IP", 
        "Surface", "Revenus", 
        "Index pésticides", "Temps")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
xtable(round(correlation, 4), 
    caption = "La correlation complete",
    align = "l|rrrrrr")
```

\FloatBarrier

#### Transformation *Within*

Les rélations entre les variables mieux ressortent pour les données transformées.

```{r include = FALSE}
correlationW = cor(dataW)
dataW$ndep = index(datap)$ndep
colnames(correlationW) = c("Quantité du vin", "IP", 
        "Surface", "Revenus", 
        "Index pésticides", "Temps")
rownames(correlationW) = c("Quantité du vin", "IP", 
        "Surface", "Revenus", 
        "Index pésticides", "Temps")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
xtable(round(correlationW, 4), 
    caption = "La correlation within",
    align = "l|rrrrrr")
```

\FloatBarrier

\newpage

## B Analyse des résultats OLS, WLS et SUR

```{r include = FALSE}
# Data transformation for systemfit
dataWX = as.data.frame(dataW)
```

```{r include = FALSE}
# equations
eqdemand = qi ~ 0 + ipi + ri
eqoffer = qi ~ 0 + ipi + si + iki 
system = list(Demande = eqdemand, Offre = eqoffer)
# OLS
ols = systemfit(system, 
    data = dataWX, 
    method = "OLS")
# WLS
wls = systemfit(system, 
    data = dataWX, 
    method = "WLS")
# SUR
sur = systemfit(system, 
    data = dataWX, 
    method = "SUR")
```

### B1 Independance des résidus  

```{r include = FALSE}
cordata = dataWX %>% 
    mutate(u1 = ols$eq[[1]]$res,
        u2 = ols$eq[[2]]$res,
        u3 = wls$eq[[1]]$res,
        u4 = wls$eq[[2]]$res,
        u5 = sur$eq[[1]]$res,
        u6 = sur$eq[[2]]$res)
cormat = cor(cordata[,-c(6,7)])[1:5, 6:11]
colnames(cormat) = c("OLS D", "OLS O", 
        "WLS D", "WLS O", 
        "SUR D", "SUR O")
rownames(cormat) = c("Vin", "IP", 
        "Surface", "Revenus", 
        "Pesticides")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
xtable(round(cormat, 4), 
    caption = "Correlation des résidus",
    align = "l|rrrrrr")
# cordata %>% ggplot(aes(x = u6, y = f6)) + geom_point() + geom_smooth()
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, fig.cap = "Les résidus contre la variable prédite"}
par(mfrow = c(1,2), cex = 0.5)
plot(y = ols$eq[[1]]$res, x = ols$eq[[1]]$fit,
    col = "blue", 
    main = "Demande", 
    ylab = "Residuals", xlab = "Fitted")
points(y = wls$eq[[1]]$res, x = wls$eq[[1]]$fit,
    col = "green4", pch = 17)
points(y = sur$eq[[1]]$res, x = sur$eq[[1]]$fit,
    col = "red", pch = 14)
plot(x = ols$eq[[2]]$res, y = ols$eq[[2]]$fit,
    col = "blue", 
    main = "Offre", 
    ylab = "Residuals", xlab = "Fitted")
points(y = wls$eq[[2]]$res, x = wls$eq[[2]]$fit,
    col = "green4", pch = 17)
points(y = sur$eq[[2]]$res, x = sur$eq[[2]]$fit,
    col = "red", pch = 14)
legend(x = "topright", legend = c("OLS", "WLS", "SUR"),
    col = c("blue", "green4", "red"), xpd = NA,
    cex = 1, lwd = 3)
```

\FloatBarrier

### B2 L'autocorrelation

```{r include = FALSE}
# Panel Durbin-Watson test
cordataDW = cordata %>% 
    group_by(ndep) %>%
    mutate(u1_diff = u1 - dplyr::lag(u1),
        u2_diff = u2 - dplyr::lag(u2),
        u3_diff = u3 - dplyr::lag(u3),
        u4_diff = u4 - dplyr::lag(u4),
        u5_diff = u5 - dplyr::lag(u5),
        u6_diff = u6 - dplyr::lag(u6)) %>%
    ungroup() %>%
    dplyr::select(contains("u")) %>%
    mutate_all(function(x) replace_na(x, 0)) %>%
    summarise_all(function(x) sum(x^2))
pDW = data.frame(ncol = 3, nrow = 2)
pDW[1,1] = cordataDW[1,1]/cordataDW[1,7]
pDW[2,1] = cordataDW[1,2]/cordataDW[1,8]
pDW[1,2] = cordataDW[1,3]/cordataDW[1,9]
pDW[2,2] = cordataDW[1,4]/cordataDW[1,10]
pDW[1,3] = cordataDW[1,5]/cordataDW[1,11]
pDW[2,3] = cordataDW[1,6]/cordataDW[1,12]
colnames(pDW) = c("OLS", "WLS", "SUR")
rownames(pDW) = c("Equation de demande", "Equation d'offre")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(pDW, 
    header = FALSE,
    title = "Les statistiques test de Durbin-Watson",
    summary = FALSE)
```

\FloatBarrier

### B3 Test de l'hétéroskedacité

```{r include = FALSE}
BartT = data.frame(ncol = 3, nrow = 2)
BartT[1,1] = ols_test_bartlett(cordata, u1, 
    group_var = ndep)$pval
BartT[2,1] = ols_test_bartlett(cordata, u2, 
    group_var = ndep)$pval
BartT[1,2] = ols_test_bartlett(cordata, u3, 
    group_var = ndep)$pval
BartT[2,2] = ols_test_bartlett(cordata, u4, 
    group_var = ndep)$pval
BartT[1,3] = ols_test_bartlett(cordata, u5, 
    group_var = ndep)$pval
BartT[2,3] = ols_test_bartlett(cordata, u6, 
    group_var = ndep)$pval
colnames(BartT) = c("OLS", "WLS", "SUR")
rownames(BartT) = c("Equation de demande", "Equation d'offre")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(BartT,
    header = FALSE,
    summary = FALSE,
    title = "Les résultat du test de Bartlett sur l'heteroscedacité")
```

\FloatBarrier

\newpage

### B4 La normalité des résidus 

\FloatBarrier

```{r include = FALSE}
ShapT = data.frame(nrow = 2, ncol = 6)
ShapT[1,1] = shapiro.test(ols$eq[[1]]$res)$p.val
ShapT[2,1] = shapiro.test(ols$eq[[2]]$res)$p.val
ShapT[1,2] = shapiro.test(wls$eq[[1]]$res)$p.val
ShapT[2,2] = shapiro.test(wls$eq[[2]]$res)$p.val
ShapT[1,3] = shapiro.test(sur$eq[[1]]$res)$p.val
ShapT[2,3] = shapiro.test(sur$eq[[2]]$res)$p.val
colnames(ShapT) = c("OLS", "WLS", "SUR")
rownames(ShapT) = c("Equation de demande", "Equation d'offre")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(ShapT,
    header = FALSE,
    summary = FALSE,
    title = "Shapiro-Wilk test de normalité des résidus")
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, fig.cap = "Les PDF des résidus"}
par(mfrow = c(1,2), cex = 0.5)
plot(density(ols$eq[[1]]$res), col = "blue", 
    main = "Demande", xlab = "Residuals")
lines(density(wls$eq[[1]]$res), col = "green4")
lines(density(sur$eq[[1]]$res), col = "red")
plot(density(sur$eq[[2]]$res), col = "red", 
    main = "Offre", xlab = "Residuals")
lines(density(wls$eq[[2]]$res), col = "green4")
lines(density(ols$eq[[2]]$res), col = "blue")
legend(x = "topright", legend = c("OLS", "WLS", "SUR"),
    col = c("blue", "green4", "red"), xpd = NA,
    cex = 1, lwd = 3)
```

\FloatBarrier

\newpage

## C Analyse des résultats 2SLS, W2SLS, 3SLS et i3SLS

\FloatBarrier

```{r include = FALSE}
# equations
eqdemand = qi ~ 0 + ipi + ri
eqoffer = qi ~ 0 + ipi + si + iki 
inst = ~ ri + si + iki
system = list(Demande = eqdemand, Offre = eqoffer)
# 2SLS
# 2SLS is an equivalent of ILS (indirect least squares)
sls2 = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "2SLS")
# 2WSLS
wsls2 = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "W2SLS")
# 3SLS (errors correction)
sls3 = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "3SLS")
# FIML (iterated 3SLS)
fiml = systemfit(system, 
    inst = inst,
    data = dataWX, 
    method = "3SLS", maxit = 1000)
```

\FloatBarrier

### C1 Independance des résidus  

```{r include = FALSE}
resdata3 = dataWX %>% 
    mutate(u1 = sls2$eq[[1]]$res,
        u2 = sls2$eq[[2]]$res,
        u3 = sls3$eq[[1]]$res,
        u4 = sls3$eq[[2]]$res,
        u5 = fiml$eq[[1]]$res,
        u6 = fiml$eq[[2]]$res)
```

```{r include = FALSE}
cormat = cor(resdata3[,-c(6,7)])
cormat = cormat[1:5, 6:11]
colnames(cormat) = c("2SLS D", "2SLS O", 
        "3SLS D", "3SLS O",
        "i3SLS D", "i3SLS O")
rownames(cormat) = c("Vin", "IP", 
        "Surface", "Revenus", 
        "Pesticides")
```

\FloatBarrier

```{r include = FALSE}
xtable(round(cormat, 4), 
    caption = "Correlation des résidus",
    align = "l|rrrrrr")
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, fig.cap = "Les résidus contre la variable prédite"}
par(mfrow = c(1,2), cex = 0.5)
plot(y = sls2$eq[[1]]$res, x = sls2$eq[[1]]$fit,
    col = "blue", 
    main = "Demande", 
    ylab = "Residuals", xlab = "Fitted")
points(y = fiml$eq[[1]]$res, x = fiml$eq[[1]]$fit,
    col = "green4", pch = 17)
points(y = sls3$eq[[1]]$res, x = sls3$eq[[1]]$fit,
    col = "red", pch = 14)
plot(y = sls2$eq[[2]]$res, x = sls2$eq[[2]]$fit,
    col = "blue", 
    main = "Offre", 
    ylab = "Residuals", xlab = "Fitted")
points(y = fiml$eq[[2]]$res, x = fiml$eq[[2]]$fit,
    col = "green4", pch = 17)
points(y = sls3$eq[[2]]$res, x = sls3$eq[[2]]$fit,
    col = "red", pch = 14)
par(xpd = NA)
legend(x = "topright", legend = c("2SLS", "i3SLS", "3SLS"),
    col = c("blue", "green4", "red"), xpd = NA,
    cex = 1, lwd = 3)
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, fig.cap = "Les résidus et les prédictions, le cas de i3SLS"}
par(mfrow = c(1,2), cex = 0.5)
plot(y = fiml$eq[[1]]$res, x = as.numeric(resdata3$qi),
    col = "red", pch = 17, 
    main = "Demande", 
    ylab = "Fitted - résidus", xlab = "Real")
points(y = fiml$eq[[1]]$fit, x = resdata3$qi, 
    col = "blue", pch = 17)
lines(y = fiml$eq[[1]]$fit + fiml$eq[[1]]$res, x = resdata3$qi,
    col = "black")
legend(x = "bottomleft", legend = c("Résidus", "Fitted", "Real"),
    col = c("red", "blue", "black"), xpd = NA,
    cex = 1, lwd = 3)
plot(y = fiml$eq[[2]]$fit, x = as.numeric(resdata3$qi), 
    col = "blue", pch = 17, 
    main = "Offre", 
    ylab = "Fitted - résidus", xlab = "Real")
points(y = fiml$eq[[2]]$res, x = resdata3$qi,
    col = "red", pch = 14)
lines(y = fiml$eq[[2]]$fit + fiml$eq[[2]]$res, x = resdata3$qi,
    col = "black")
```

\FloatBarrier

\newpage

### C2 L'autocorrelation 

```{r include = FALSE}
# Panel Durbin-Watson test
# Create dataframe
resdataDW = resdata3 %>% 
    group_by(ndep) %>%
    mutate(u1_diff = u1 - dplyr::lag(u1),
        u2_diff = u2 - dplyr::lag(u2),
        u3_diff = u3 - dplyr::lag(u3),
        u4_diff = u4 - dplyr::lag(u4),
        u5_diff = u5 - dplyr::lag(u5),
        u6_diff = u6 - dplyr::lag(u6)) %>%
    ungroup() %>%
    dplyr::select(contains("u")) %>%
    mutate_all(function(x) replace_na(x, 0)) %>%
    summarise_all(function(x) sum(x^2))
# Calculate test statistics
pDW = data.frame(ncol = 3, nrow = 2)
pDW[1,1] = resdataDW[1,1]/resdataDW[1,7]
pDW[2,1] = resdataDW[1,2]/resdataDW[1,8]
pDW[1,2] = resdataDW[1,3]/resdataDW[1,9]
pDW[2,2] = resdataDW[1,4]/resdataDW[1,10]
pDW[1,3] = resdataDW[1,5]/resdataDW[1,11]
pDW[2,3] = resdataDW[1,6]/resdataDW[1,12]
# Rename
colnames(pDW) = c("2SLS", "3SLS", "i3SLS")
rownames(pDW) = c("Equation de demande", "Equation d'offre")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(pDW,
    header = FALSE,
    title = "Les resultats du test de Durbin-Watson",
    summary = FALSE)
```

\FloatBarrier

### C3 Test de l'hétéroskedacité

```{r include = FALSE}
resdata3 = dataWX %>% 
    mutate(u1 = sls2$eq[[1]]$res,
        u2 = sls2$eq[[2]]$res,
        u3 = sls3$eq[[1]]$res,
        u4 = sls3$eq[[2]]$res,
        u5 = fiml$eq[[1]]$res,
        u6 = fiml$eq[[2]]$res)
```

```{r include = FALSE}
BartT = data.frame(ncol = 3, nrow = 2)
BartT[1,1] = ols_test_bartlett(resdata3, u1, group_var = ndep)$pval
BartT[2,1] = ols_test_bartlett(resdata3, u2, group_var = ndep)$pval
BartT[1,2] = ols_test_bartlett(resdata3, u3, group_var = ndep)$pval
BartT[2,2] = ols_test_bartlett(resdata3, u4, group_var = ndep)$pval
BartT[1,3] = ols_test_bartlett(resdata3, u5, group_var = ndep)$pval
BartT[2,3] = ols_test_bartlett(resdata3, u6, group_var = ndep)$pval
colnames(BartT) = c("2SLS", "3SLS", "i3SLS")
rownames(BartT) = c("Equation de demande", "Equation d'offre")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(BartT,
    summary = FALSE,
    title = "Test de Bartlett sur l'heterockedacité")
```

\FloatBarrier

\newpage

### C4 La normalité des résidus 

\FloatBarrier

```{r include = FALSE}
ShapT = data.frame(nrow = 2, ncol = 6)
ShapT[1,1] = shapiro.test(sls2$eq[[1]]$res)$p.val
ShapT[2,1] = shapiro.test(sls2$eq[[2]]$res)$p.val
ShapT[1,2] = shapiro.test(sls3$eq[[1]]$res)$p.val
ShapT[2,2] = shapiro.test(sls3$eq[[2]]$res)$p.val
ShapT[1,3] = shapiro.test(fiml$eq[[1]]$res)$p.val
ShapT[2,3] = shapiro.test(fiml$eq[[2]]$res)$p.val
colnames(ShapT) = c("2SLS", "3SLS", "i3SLS")
rownames(ShapT) = c("Equation de demande", "Equation d'offre")
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(ShapT,
    header = FALSE,
    summary = FALSE,
    title = "Shapiro-Wilk test de normalité")
```

\FloatBarrier

\FloatBarrier

```{r echo = FALSE, fig.cap = "Les PDF des résidus"}
par(mfrow = c(1,2), cex = 0.5)
plot(density(sls2$eq[[1]]$res), col = "blue", 
    main = "Demande", xlab = "Residuals")
lines(density(fiml$eq[[1]]$res), col = "green4")
lines(density(sls3$eq[[1]]$res), col = "red")
plot(density(sls3$eq[[2]]$res), col = "red", 
    main = "Offre", xlab = "Residuals")
lines(density(fiml$eq[[2]]$res), col = "green4")
lines(density(sls2$eq[[2]]$res), col = "blue")
legend(x = "topright", legend = c("2SLS", "i3SLS", "3SLS"),
    col = c("blue", "green4", "red"), xpd = NA,
    cex = 1, lwd = 3)
```

\FloatBarrier

### C5 Comparaison des modèles  

\FloatBarrier

```{r include = FALSE}
h1 = hausman.systemfit(sls2, sls3) # p = 1, 3SLS inconsistent
# 2SLS estimator is consistent
h2 = hausman.systemfit(sls2, fiml)
res = data.frame(Test = c("2SLS contre 3SLS", "2SLS contre i3SLS"),
    Resultats = c(h1$p.val, h2$p.val))
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(res, 
    title = "Hausman 3SLS consistency test",
    summary = F, 
    header = F)
``` 

\FloatBarrier

```{r include = FALSE}
# - Linear test :
# What should be tested ???
# linearHypothesis(sls2, 
```
\FloatBarrier

```{r include = FALSE}
lr = lrtest(sls2, sls3, fiml)
xtable(round(lr, 4), 
    caption = "ML test de spécification",
    align = "c|ccccc")
```

\FloatBarrier

\newpage 

## D Clusterisation 

### D1 *Between* transformation 

Les groupes sont définies par des caractéristiques suivantes :

\FloatBarrier

```{r echo = FALSE, results = "asis"}
clusters1 = cbind(fit1$centers, fit1$size)
names(clusters1) = c("Groupe", "Quantité", "IP", 
    "Surface", "Revenus", "Index pesticides", "n ")
stargazer(clusters1, 
    title = "Les centres des clusters",
    summary = F, 
    header = F)
```

\FloatBarrier

### D2 *Within* transformation 

#### Les centres

Les groupes sont définies par des caractéristiques suivantes :

```{r include = FALSE}
names(clusters2)[(ncol(clusters2)-1):ncol(clusters2)] = c("n", "k")
c1 = clusters2 %>%
    dplyr::select(qi, ipi, si, ri, iki, n, k) %>%
    mutate(t = 1)
c2 = clusters2 %>%
    dplyr::select(ends_with("t2"), n, k) %>%
    mutate(t = 2)
names(c2)[1:5] = c("qi", "ipi", "si", "ri", "iki")
c3 = clusters2 %>%
    dplyr::select(ends_with("t3"), n, k) %>%
    mutate(t = 3)
names(c3)[1:5] = c("qi", "ipi", "si", "ri", "iki")
c4 = clusters2 %>%
    dplyr::select(ends_with("t4"), n, k) %>%
    mutate(t = 4)
names(c4)[1:5] = c("qi", "ipi", "si", "ri", "iki")
c5 = clusters2 %>%
    dplyr::select(ends_with("t5"), n, k) %>%
    mutate(t = 5)
names(c5)[1:5] = c("qi", "ipi", "si", "ri", "iki")
centers2 = rbind(c1, c2, c3, c4, c5) %>% 
    group_by(k) %>% 
    arrange(t, .by_group = T)
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
# Print results
stargazer(round(centers2, 6), 
    title = "Les centres des clusters",
    summary = F, 
    header = F)
```

\FloatBarrier

#### Representation graphique

```{r echo = FALSE, results = "asis"}
# Plot 
p1 = centers2 %>% ggplot(aes(y = qi, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("Quantité") +
    ggtitle("Q~T") +
    pres_theme
p2 = centers2 %>% ggplot(aes(y = ipi, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("IP") +
    ggtitle("IP~T") +
    pres_theme
p3 = centers2 %>% ggplot(aes(y = iki, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("Index pesticides") +
    ggtitle("IK~T") +
    pres_theme
p4 = centers2 %>% ggplot(aes(y = ri, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("Revenus") +
    ggtitle("R~T") +
    pres_theme
# Arrange
grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)
```

### D3 Cas d'information complete 

#### Les centres

Les groupes sont définies par des caractéristiques suivantes :

```{r include = FALSE}
names(clusters3)[(ncol(clusters3)-1):ncol(clusters3)] = c("n", "k")
c1 = clusters3 %>%
    dplyr::select(qi, ipi, si, ri, iki, n, k) %>%
    mutate(t = 1)
c2 = clusters3 %>%
    dplyr::select(ends_with("t2"), n, k) %>%
    mutate(t = 2)
names(c2)[1:5] = c("qi", "ipi", "si", "ri", "iki")
c3 = clusters3 %>%
    dplyr::select(ends_with("t3"), n, k) %>%
    mutate(t = 3)
names(c3)[1:5] = c("qi", "ipi", "si", "ri", "iki")
c4 = clusters3 %>%
    dplyr::select(ends_with("t4"), n, k) %>%
    mutate(t = 4)
names(c4)[1:5] = c("qi", "ipi", "si", "ri", "iki")
c5 = clusters3 %>%
    dplyr::select(ends_with("t5"), n, k) %>%
    mutate(t = 5)
names(c5)[1:5] = c("qi", "ipi", "si", "ri", "iki")
centers3 = rbind(c1, c2, c3, c4, c5) %>% 
    group_by(k) %>% 
    arrange(t, .by_group = T)
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
# Print 
stargazer(round(centers3, 6), 
    title = "Les centres des clusters",
    summary = F, 
    header = F)
```

\FloatBarrier

#### Representation graphique

\FloatBarrier

```{r echo = FALSE, results = "asis"}
# Plot 
p1 = centers3 %>% ggplot(aes(y = qi, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("Quantité") +
    ggtitle("Q~T") +
    pres_theme
p2 = centers3 %>% ggplot(aes(y = ipi, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("IP") +
    ggtitle("IP~T") +
    pres_theme
p3 = centers3 %>% ggplot(aes(y = iki, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("Index pesticides") +
    ggtitle("IK~T") +
    pres_theme
p4 = centers3 %>% ggplot(aes(y = ri, x = t, col = as.factor(k))) + 
    geom_path() + geom_point() +
    xlab("Temps") + ylab("Revenus") +
    ggtitle("R~T") +
    pres_theme
# Arrange
grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)
```

\FloatBarrier

## E Analyse des résultats pour information clusterisée OLS, 2SLS et 3SLS

Etudions la validité du modèle 3SLS : 

```{r include = FALSE}
resdata3x = dataWZ %>% 
    mutate(u1 = olsx$eq[[1]]$res,
        u2 = olsx$eq[[2]]$res,
        u3 = sls2x$eq[[1]]$res,
        u4 = sls2x$eq[[2]]$res,
        u5 = sls3x$eq[[1]]$res,
        u6 = sls3x$eq[[2]]$res)
```

\FloatBarrier

```{r include = FALSE}
h1 = hausman.systemfit(sls2x, sls3x) # p = 1, 3SLS inconsistent
# 2SLS estimator is consistent
res = data.frame(Test = c("2SLS contre 3SLS"),
    Resultats = c(h1$p.val)) 
```

\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(res, 
    title = "Hausman 3SLS consistency test",
    summary = F, 
    header = F)
```

La normalité des résidus :

```{r include = FALSE}
ShapT = data.frame(nrow = 2, ncol = 6)
ShapT[1,1] = shapiro.test(olsx$eq[[1]]$res)$p.val
ShapT[2,1] = shapiro.test(olsx$eq[[2]]$res)$p.val
ShapT[1,2] = shapiro.test(sls2x$eq[[1]]$res)$p.val
ShapT[2,2] = shapiro.test(sls2x$eq[[2]]$res)$p.val
ShapT[1,3] = shapiro.test(sls3x$eq[[1]]$res)$p.val
ShapT[2,3] = shapiro.test(sls3x$eq[[2]]$res)$p.val
colnames(ShapT) = c("OLS", "2SLS", "3SLS")
rownames(ShapT) = c("Equation de demande", "Equation d'offre")
```
\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(ShapT,
    summary = FALSE,
    title = "Shapiro-Wilk normality test",
    header = F)
```

\FloatBarrier

L'heteroscedacité :

\FloatBarrier

```{r include = FALSE}
BartT = data.frame(ncol = 3, nrow = 2)
BartT[1,1] = ols_test_bartlett(resdata3x, u1, 
    group_var = ndep)$pval
BartT[2,1] = ols_test_bartlett(resdata3x, u2, 
    group_var = ndep)$pval
BartT[1,2] = ols_test_bartlett(resdata3x, u3, 
    group_var = ndep)$pval
BartT[2,2] = ols_test_bartlett(resdata3x, u4, 
    group_var = ndep)$pval
BartT[1,3] = ols_test_bartlett(resdata3x, u5, 
    group_var = ndep)$pval
BartT[2,3] = ols_test_bartlett(resdata3x, u6, 
    group_var = ndep)$pval
colnames(BartT) = c("OLS", "2SLS", "3SLS")
rownames(BartT) = c("Equation de demande", "Equation d'offre")
```
\FloatBarrier

```{r echo = FALSE, results = "asis"}
stargazer(BartT,
    summary = FALSE,
    title = "Bartlett heteroscedasticity test",
    header = F)
```

\FloatBarrier

Les PDF des résidus :

\FloatBarrier

```{r echo = FALSE}
par(mfrow = c(1,2), cex = 0.5)
plot(density(olsx$eq[[1]]$res), col = "blue", 
    main = "Demande", xlab = "Residuals")
lines(density(sls2x$eq[[1]]$res), col = "green4")
lines(density(sls3x$eq[[1]]$res), col = "red")
plot(density(sls3x$eq[[2]]$res), col = "red", 
    main = "Offre", xlab = "Residuals")
lines(density(sls2x$eq[[2]]$res), col = "green4")
lines(density(olsx$eq[[2]]$res), col = "blue")
legend(x = "topright", legend = c("OLS", "2SLS", "3SLS"),
    col = c("blue", "green4", "red"), xpd = NA,
    cex = 1, lwd = 3)
```

\FloatBarrier

Les résidus contre les variables prédites : 

\FloatBarrier

```{r echo = FALSE}
par(mfrow = c(1,2), cex = 0.5)
plot(y = sls3x$eq[[1]]$res, x = sls3x$eq[[1]]$fit,
    col = "red", 
    main = "Demande", 
    ylab = "Residuals", xlab = "Fitted")
points(y = sls2x$eq[[1]]$res, x = sls2x$eq[[1]]$fit,
    col = "green4", pch = 17)
points(y = olsx$eq[[1]]$res, x = olsx$eq[[1]]$fit,
    col = "blue", pch = 14)
plot(y = sls3x$eq[[2]]$res, x = sls3x$eq[[2]]$fit,
    col = "red", 
    main = "Offre", 
    ylab = "Residuals", xlab = "Fitted")
points(y = sls2x$eq[[2]]$res, x = sls2x$eq[[2]]$fit,
    col = "green4", pch = 17)
points(y = olsx$eq[[2]]$res, x = olsx$eq[[2]]$fit,
    col = "blue", pch = 14)
par(xpd = NA)
legend(x = "topright", legend = c("OLS", "2SLS", "3SLS"),
    col = c("blue", "green4", "red"), xpd = NA,
    cex = 1, lwd = 3)
```

\FloatBarrier

\newpage

## F Dictionnaire des variables 

Finalement, nous offrons au lecteur un tableau de reference pour notre base des données finale.

\FloatBarrier

\begin{table}[!htbp]
  \centering
\caption{Ditionnaire des varibales}
\begin{tabular}{c|l}
  \noindent\rule[0.5ex]{\linewidth}{1pt}
  Variable & Description \\
  \noindent\rule[0.5ex]{\linewidth}{1pt}
année & année \\
ndep & numéro de département \\
si & superficie de vigne sans indication géographique en hectare en log \\
qi & quantité de vins produits en hectolitre en log \\
ipi & indice des prix du vin sans IG déflatés en log  \\
ri & revenu disponible brut des ménages français déflatés en log \\
iki & indice de quantité de pesticides achetés en log \\
t & la tendance temporelle \\
\noindent\rule[0.5ex]{\linewidth}{1pt}
\end{tabular}
\end{table}

\FloatBarrier

\newpage

# References 

```{r eval = FALSE, include = FALSE}
#####################################################
###################   References   ##################
#####################################################
```